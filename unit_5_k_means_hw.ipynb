{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w4mcI7K_hQ9"
      },
      "source": [
        "# Unit-5-HW: Finding the Centroid in Shakespeare's Plays\n",
        "\n",
        "This assignment was borrowed from Dr. Paul's section on Exploring Machine Learning. I think it presents a unique showcase on how k means can be used to categorize data. A lot of the setup is done for you in this notebook to give you a feel of what it might look like for your project.\n",
        "\n",
        "There are no pytest cases for this homework since it was borrowed from Dr. Paul. Please see the rubric at the bottom of the file for grading. I have also left this notebook a bit more open than usual, if you have questions about grading please let me know. Please push this notebook to your github repo like any other assignment. \n",
        "\n",
        "\n",
        "Citations:\n",
        "\n",
        "This notebook is based on John Lad's exploration which I encourage you to explore and take inspiration: https://jrladd.com/statistical_modeling\n",
        "\n",
        "**Data:** [Shakespeare's plays](https://drive.google.com/drive/folders/1FOAnnR5ekwJsWLXsSVa_5fHOnShLLGc1?usp=sharing)\n",
        "\n",
        "Data Source: [Text files provided by the Folger Shakespeare Library](https://www.folger.edu/explore/shakespeares-works/download/#titus-andronicus) (Washington D.C.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJvN6vOFLsk"
      },
      "source": [
        "# The Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRV2oWRADCQx"
      },
      "source": [
        "Shakespeare and data go together, believe it or not. The field of bibliography is the collecting of data on books, and people have been doing it for Shakespeare's plays and poems for centuries.\n",
        "\n",
        "One of the major questions we have about each of Shakespeare's plays is this: are they Comedy, History, Tragedy, Romance, and Tragicomedy? You'll use data to explore a group of the plays you choose, or all 37 of them!\n",
        "\n",
        "Specifically, we will use the unsupervised learning method, K-means clustering, to categorize Shakespeare's plays into groups.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA8FcrRZFTJW"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "First, you will have to turn Shakespeare's plays into numbers! Surprisingly, we now know that we can classify texts by knowing just the 100 most commonly occuring words. So, for this small project, you will begin by using a method called Term Frequency - Inverse Document Frequency (tf-idf) to **count the words** in Shakespeare's plays.\n",
        "\n",
        "[Here](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf) is a description of an application of the Tf-idf method, and [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) is its documentation on the Tf-idf method. Since we did not go over this method in class, and because we are moving away from the more guided worksheets, I am giving you most time in the assignment to learn about the method. Importantly, **Tf-idf measures how relevant a word is in a series of words by determining how much information it provides**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnGbb--CuWK8"
      },
      "source": [
        "# Implement the Tf-idf Solution\n",
        "\n",
        "The celles of code, below, are written by a fellow scholar of early modern literature, John Ladd. We are going to study his approach, however, you are welcome to find a new approach, as you are also welcome to use data for clustering other than Shakespeare's plays for your completion of this assignment. The questions at the end of the notebook ask you to evaluate Ladd's model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hbpfihIvuQlJ"
      },
      "outputs": [],
      "source": [
        "import requests, re, zipfile, io\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QywZHOcBvnCL"
      },
      "source": [
        "#Empty lists for titles and texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-Eh4XChkvrLb"
      },
      "outputs": [],
      "source": [
        "titles = [] #use as row labels\n",
        "texts = [] #the data we will analyze\n",
        "\n",
        "shakeszip = requests.get(\"https://jrladd.com/CIS241/data/shakespeare.zip\") #we will use John Ladd's zip file for this part of the activity.\n",
        "#However, the data is drawn from the Folger Shakespeare Library's website, which the first cell in this notebook links to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rTUWox8vyoZ"
      },
      "source": [
        "# Unzip the Folder, Get All the Files Out, and Save the Play Titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A2jNzzyhuxlo"
      },
      "outputs": [],
      "source": [
        "#path_to_file = \"\"\n",
        "#df = pd.read_csv(path_to_file)\n",
        "\n",
        "with zipfile.ZipFile(io.BytesIO(shakeszip.content)) as myzip: # Look inside our zipfile\n",
        "    for i in myzip.infolist(): # Loop through each file\n",
        "        if i.is_dir() == False and i.filename.startswith('__MACOSX') == False: # Filter out the pointless duplicates\n",
        "            titles.append(re.split(r\"/|_TXT\",i.filename)[1]) # Add titles to list\n",
        "            texts.append(myzip.read(i.filename)) # Add the text to list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxAoFF7ywiCK"
      },
      "source": [
        "# Create a Vectorizer Instance for 100 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ATBc05HYwn-7"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGKrlesSw1Y3"
      },
      "source": [
        "# Transform Files into Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h1iMFKfwwyTQ"
      },
      "outputs": [],
      "source": [
        "shakespeare = vectorizer.fit_transform(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvhBQXq8w8nb"
      },
      "source": [
        "# Turn vectorizer results into readable dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_dKmF_RJxF_r",
        "outputId": "27b8d9e3-2a3e-4b31-bbf3-9cd9d4eab917"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>all</th>\n",
              "      <th>am</th>\n",
              "      <th>an</th>\n",
              "      <th>and</th>\n",
              "      <th>are</th>\n",
              "      <th>as</th>\n",
              "      <th>at</th>\n",
              "      <th>be</th>\n",
              "      <th>but</th>\n",
              "      <th>by</th>\n",
              "      <th>...</th>\n",
              "      <th>where</th>\n",
              "      <th>which</th>\n",
              "      <th>who</th>\n",
              "      <th>why</th>\n",
              "      <th>will</th>\n",
              "      <th>with</th>\n",
              "      <th>would</th>\n",
              "      <th>yet</th>\n",
              "      <th>you</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>much-ado-about-nothing</th>\n",
              "      <td>0.054234</td>\n",
              "      <td>0.046922</td>\n",
              "      <td>0.052406</td>\n",
              "      <td>0.396703</td>\n",
              "      <td>0.056063</td>\n",
              "      <td>0.087141</td>\n",
              "      <td>0.034125</td>\n",
              "      <td>0.110297</td>\n",
              "      <td>0.106031</td>\n",
              "      <td>0.060938</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>0.034125</td>\n",
              "      <td>0.019500</td>\n",
              "      <td>0.029859</td>\n",
              "      <td>0.120047</td>\n",
              "      <td>0.125531</td>\n",
              "      <td>0.053625</td>\n",
              "      <td>0.020719</td>\n",
              "      <td>0.303469</td>\n",
              "      <td>0.111516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>richard-iii</th>\n",
              "      <td>0.064522</td>\n",
              "      <td>0.025982</td>\n",
              "      <td>0.012991</td>\n",
              "      <td>0.403587</td>\n",
              "      <td>0.041138</td>\n",
              "      <td>0.059759</td>\n",
              "      <td>0.042437</td>\n",
              "      <td>0.095700</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.073183</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020786</td>\n",
              "      <td>0.033344</td>\n",
              "      <td>0.026848</td>\n",
              "      <td>0.019920</td>\n",
              "      <td>0.070151</td>\n",
              "      <td>0.119084</td>\n",
              "      <td>0.027281</td>\n",
              "      <td>0.020353</td>\n",
              "      <td>0.169749</td>\n",
              "      <td>0.118651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the-winters-tale</th>\n",
              "      <td>0.054603</td>\n",
              "      <td>0.037802</td>\n",
              "      <td>0.027826</td>\n",
              "      <td>0.350719</td>\n",
              "      <td>0.057753</td>\n",
              "      <td>0.121807</td>\n",
              "      <td>0.031502</td>\n",
              "      <td>0.132307</td>\n",
              "      <td>0.117606</td>\n",
              "      <td>0.067729</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014701</td>\n",
              "      <td>0.063528</td>\n",
              "      <td>0.022051</td>\n",
              "      <td>0.014176</td>\n",
              "      <td>0.060378</td>\n",
              "      <td>0.109206</td>\n",
              "      <td>0.040952</td>\n",
              "      <td>0.026776</td>\n",
              "      <td>0.247813</td>\n",
              "      <td>0.138082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>richard-ii</th>\n",
              "      <td>0.061009</td>\n",
              "      <td>0.024290</td>\n",
              "      <td>0.020901</td>\n",
              "      <td>0.415197</td>\n",
              "      <td>0.048016</td>\n",
              "      <td>0.080215</td>\n",
              "      <td>0.035588</td>\n",
              "      <td>0.094337</td>\n",
              "      <td>0.084734</td>\n",
              "      <td>0.053665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021466</td>\n",
              "      <td>0.040108</td>\n",
              "      <td>0.018642</td>\n",
              "      <td>0.016382</td>\n",
              "      <td>0.054795</td>\n",
              "      <td>0.145743</td>\n",
              "      <td>0.025420</td>\n",
              "      <td>0.028245</td>\n",
              "      <td>0.091513</td>\n",
              "      <td>0.071177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>henry-vi-part-3</th>\n",
              "      <td>0.060131</td>\n",
              "      <td>0.023441</td>\n",
              "      <td>0.019364</td>\n",
              "      <td>0.483085</td>\n",
              "      <td>0.037709</td>\n",
              "      <td>0.074399</td>\n",
              "      <td>0.042805</td>\n",
              "      <td>0.105993</td>\n",
              "      <td>0.100388</td>\n",
              "      <td>0.056054</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022931</td>\n",
              "      <td>0.019874</td>\n",
              "      <td>0.014778</td>\n",
              "      <td>0.036180</td>\n",
              "      <td>0.070322</td>\n",
              "      <td>0.141155</td>\n",
              "      <td>0.024460</td>\n",
              "      <td>0.025989</td>\n",
              "      <td>0.102936</td>\n",
              "      <td>0.072870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the-two-noble-kinsmen</th>\n",
              "      <td>0.080347</td>\n",
              "      <td>0.043826</td>\n",
              "      <td>0.024160</td>\n",
              "      <td>0.472531</td>\n",
              "      <td>0.078100</td>\n",
              "      <td>0.088213</td>\n",
              "      <td>0.037083</td>\n",
              "      <td>0.116307</td>\n",
              "      <td>0.096079</td>\n",
              "      <td>0.057872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.029779</td>\n",
              "      <td>0.037083</td>\n",
              "      <td>0.019104</td>\n",
              "      <td>0.020227</td>\n",
              "      <td>0.065177</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.047197</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.247784</td>\n",
              "      <td>0.103946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timon-of-athens</th>\n",
              "      <td>0.097536</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.400568</td>\n",
              "      <td>0.064031</td>\n",
              "      <td>0.062542</td>\n",
              "      <td>0.037972</td>\n",
              "      <td>0.096047</td>\n",
              "      <td>0.102003</td>\n",
              "      <td>0.047651</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014146</td>\n",
              "      <td>0.035738</td>\n",
              "      <td>0.023826</td>\n",
              "      <td>0.023826</td>\n",
              "      <td>0.048396</td>\n",
              "      <td>0.127318</td>\n",
              "      <td>0.040206</td>\n",
              "      <td>0.023081</td>\n",
              "      <td>0.214430</td>\n",
              "      <td>0.099770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the-merchant-of-venice</th>\n",
              "      <td>0.050456</td>\n",
              "      <td>0.044589</td>\n",
              "      <td>0.032268</td>\n",
              "      <td>0.361405</td>\n",
              "      <td>0.059256</td>\n",
              "      <td>0.111472</td>\n",
              "      <td>0.036962</td>\n",
              "      <td>0.109712</td>\n",
              "      <td>0.106779</td>\n",
              "      <td>0.062190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016427</td>\n",
              "      <td>0.035788</td>\n",
              "      <td>0.027575</td>\n",
              "      <td>0.023468</td>\n",
              "      <td>0.077444</td>\n",
              "      <td>0.114406</td>\n",
              "      <td>0.042242</td>\n",
              "      <td>0.020534</td>\n",
              "      <td>0.265187</td>\n",
              "      <td>0.102672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loves-labors-lost</th>\n",
              "      <td>0.054099</td>\n",
              "      <td>0.033061</td>\n",
              "      <td>0.042077</td>\n",
              "      <td>0.349241</td>\n",
              "      <td>0.060711</td>\n",
              "      <td>0.093772</td>\n",
              "      <td>0.039673</td>\n",
              "      <td>0.084756</td>\n",
              "      <td>0.095575</td>\n",
              "      <td>0.073335</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019235</td>\n",
              "      <td>0.040875</td>\n",
              "      <td>0.015028</td>\n",
              "      <td>0.022241</td>\n",
              "      <td>0.106395</td>\n",
              "      <td>0.100985</td>\n",
              "      <td>0.031858</td>\n",
              "      <td>0.010820</td>\n",
              "      <td>0.204375</td>\n",
              "      <td>0.096778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>troilus-and-cressida</th>\n",
              "      <td>0.075956</td>\n",
              "      <td>0.018612</td>\n",
              "      <td>0.037727</td>\n",
              "      <td>0.412981</td>\n",
              "      <td>0.053320</td>\n",
              "      <td>0.122737</td>\n",
              "      <td>0.043260</td>\n",
              "      <td>0.098592</td>\n",
              "      <td>0.103119</td>\n",
              "      <td>0.055332</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019618</td>\n",
              "      <td>0.021630</td>\n",
              "      <td>0.024648</td>\n",
              "      <td>0.031690</td>\n",
              "      <td>0.076459</td>\n",
              "      <td>0.115192</td>\n",
              "      <td>0.038230</td>\n",
              "      <td>0.023642</td>\n",
              "      <td>0.228875</td>\n",
              "      <td>0.064387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a-midsummer-nights-dream</th>\n",
              "      <td>0.074013</td>\n",
              "      <td>0.044093</td>\n",
              "      <td>0.025983</td>\n",
              "      <td>0.466910</td>\n",
              "      <td>0.053541</td>\n",
              "      <td>0.115743</td>\n",
              "      <td>0.033857</td>\n",
              "      <td>0.082674</td>\n",
              "      <td>0.096059</td>\n",
              "      <td>0.057478</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022834</td>\n",
              "      <td>0.029920</td>\n",
              "      <td>0.009448</td>\n",
              "      <td>0.017322</td>\n",
              "      <td>0.090547</td>\n",
              "      <td>0.142514</td>\n",
              "      <td>0.031495</td>\n",
              "      <td>0.021259</td>\n",
              "      <td>0.217314</td>\n",
              "      <td>0.096059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>henry-iv-part-1</th>\n",
              "      <td>0.070749</td>\n",
              "      <td>0.035632</td>\n",
              "      <td>0.043379</td>\n",
              "      <td>0.449279</td>\n",
              "      <td>0.041829</td>\n",
              "      <td>0.127554</td>\n",
              "      <td>0.045961</td>\n",
              "      <td>0.100184</td>\n",
              "      <td>0.092438</td>\n",
              "      <td>0.064035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017558</td>\n",
              "      <td>0.025304</td>\n",
              "      <td>0.015492</td>\n",
              "      <td>0.029436</td>\n",
              "      <td>0.070232</td>\n",
              "      <td>0.109996</td>\n",
              "      <td>0.029952</td>\n",
              "      <td>0.022722</td>\n",
              "      <td>0.180744</td>\n",
              "      <td>0.067134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>henry-vi-part-1</th>\n",
              "      <td>0.050526</td>\n",
              "      <td>0.031504</td>\n",
              "      <td>0.018427</td>\n",
              "      <td>0.454733</td>\n",
              "      <td>0.043987</td>\n",
              "      <td>0.070736</td>\n",
              "      <td>0.041015</td>\n",
              "      <td>0.137312</td>\n",
              "      <td>0.090947</td>\n",
              "      <td>0.060631</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020210</td>\n",
              "      <td>0.024966</td>\n",
              "      <td>0.013077</td>\n",
              "      <td>0.009511</td>\n",
              "      <td>0.088569</td>\n",
              "      <td>0.161088</td>\n",
              "      <td>0.023777</td>\n",
              "      <td>0.021994</td>\n",
              "      <td>0.120073</td>\n",
              "      <td>0.091541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>henry-v</th>\n",
              "      <td>0.062621</td>\n",
              "      <td>0.011884</td>\n",
              "      <td>0.029254</td>\n",
              "      <td>0.458000</td>\n",
              "      <td>0.044794</td>\n",
              "      <td>0.090503</td>\n",
              "      <td>0.035653</td>\n",
              "      <td>0.080904</td>\n",
              "      <td>0.075419</td>\n",
              "      <td>0.047994</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010056</td>\n",
              "      <td>0.031082</td>\n",
              "      <td>0.015998</td>\n",
              "      <td>0.012341</td>\n",
              "      <td>0.085018</td>\n",
              "      <td>0.114272</td>\n",
              "      <td>0.029711</td>\n",
              "      <td>0.017826</td>\n",
              "      <td>0.170036</td>\n",
              "      <td>0.105130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pericles</th>\n",
              "      <td>0.064438</td>\n",
              "      <td>0.028325</td>\n",
              "      <td>0.024076</td>\n",
              "      <td>0.374592</td>\n",
              "      <td>0.063022</td>\n",
              "      <td>0.094179</td>\n",
              "      <td>0.057357</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.106217</td>\n",
              "      <td>0.072228</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027616</td>\n",
              "      <td>0.051692</td>\n",
              "      <td>0.046735</td>\n",
              "      <td>0.018411</td>\n",
              "      <td>0.067979</td>\n",
              "      <td>0.114006</td>\n",
              "      <td>0.040362</td>\n",
              "      <td>0.033281</td>\n",
              "      <td>0.256337</td>\n",
              "      <td>0.138082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the-merry-wives-of-windsor</th>\n",
              "      <td>0.046426</td>\n",
              "      <td>0.045831</td>\n",
              "      <td>0.030951</td>\n",
              "      <td>0.392834</td>\n",
              "      <td>0.039879</td>\n",
              "      <td>0.108922</td>\n",
              "      <td>0.049402</td>\n",
              "      <td>0.126183</td>\n",
              "      <td>0.073805</td>\n",
              "      <td>0.059520</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012499</td>\n",
              "      <td>0.015475</td>\n",
              "      <td>0.012499</td>\n",
              "      <td>0.029165</td>\n",
              "      <td>0.131540</td>\n",
              "      <td>0.114279</td>\n",
              "      <td>0.044640</td>\n",
              "      <td>0.018451</td>\n",
              "      <td>0.328552</td>\n",
              "      <td>0.122612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>as-you-like-it</th>\n",
              "      <td>0.054894</td>\n",
              "      <td>0.042314</td>\n",
              "      <td>0.023444</td>\n",
              "      <td>0.400838</td>\n",
              "      <td>0.063471</td>\n",
              "      <td>0.172686</td>\n",
              "      <td>0.033737</td>\n",
              "      <td>0.108072</td>\n",
              "      <td>0.110931</td>\n",
              "      <td>0.052606</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010864</td>\n",
              "      <td>0.033737</td>\n",
              "      <td>0.023444</td>\n",
              "      <td>0.032593</td>\n",
              "      <td>0.093205</td>\n",
              "      <td>0.114362</td>\n",
              "      <td>0.041170</td>\n",
              "      <td>0.029734</td>\n",
              "      <td>0.302487</td>\n",
              "      <td>0.110359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>king-john</th>\n",
              "      <td>0.056905</td>\n",
              "      <td>0.024215</td>\n",
              "      <td>0.030269</td>\n",
              "      <td>0.427393</td>\n",
              "      <td>0.032690</td>\n",
              "      <td>0.079909</td>\n",
              "      <td>0.038138</td>\n",
              "      <td>0.104124</td>\n",
              "      <td>0.092017</td>\n",
              "      <td>0.064170</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018767</td>\n",
              "      <td>0.036928</td>\n",
              "      <td>0.022399</td>\n",
              "      <td>0.013924</td>\n",
              "      <td>0.069013</td>\n",
              "      <td>0.132577</td>\n",
              "      <td>0.026031</td>\n",
              "      <td>0.018161</td>\n",
              "      <td>0.137420</td>\n",
              "      <td>0.107756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cymbeline</th>\n",
              "      <td>0.058577</td>\n",
              "      <td>0.042602</td>\n",
              "      <td>0.018880</td>\n",
              "      <td>0.353884</td>\n",
              "      <td>0.056641</td>\n",
              "      <td>0.214944</td>\n",
              "      <td>0.039697</td>\n",
              "      <td>0.125384</td>\n",
              "      <td>0.110377</td>\n",
              "      <td>0.063418</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026142</td>\n",
              "      <td>0.060030</td>\n",
              "      <td>0.028562</td>\n",
              "      <td>0.016460</td>\n",
              "      <td>0.057609</td>\n",
              "      <td>0.106988</td>\n",
              "      <td>0.038245</td>\n",
              "      <td>0.029047</td>\n",
              "      <td>0.207199</td>\n",
              "      <td>0.112797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alls-well-that-ends-well</th>\n",
              "      <td>0.056709</td>\n",
              "      <td>0.042254</td>\n",
              "      <td>0.031135</td>\n",
              "      <td>0.344704</td>\n",
              "      <td>0.050594</td>\n",
              "      <td>0.080060</td>\n",
              "      <td>0.037806</td>\n",
              "      <td>0.108415</td>\n",
              "      <td>0.104523</td>\n",
              "      <td>0.053930</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020571</td>\n",
              "      <td>0.048370</td>\n",
              "      <td>0.012787</td>\n",
              "      <td>0.016679</td>\n",
              "      <td>0.087288</td>\n",
              "      <td>0.092848</td>\n",
              "      <td>0.046702</td>\n",
              "      <td>0.027243</td>\n",
              "      <td>0.274095</td>\n",
              "      <td>0.122314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>henry-viii</th>\n",
              "      <td>0.082688</td>\n",
              "      <td>0.033688</td>\n",
              "      <td>0.027052</td>\n",
              "      <td>0.379753</td>\n",
              "      <td>0.066865</td>\n",
              "      <td>0.085240</td>\n",
              "      <td>0.042365</td>\n",
              "      <td>0.095449</td>\n",
              "      <td>0.068396</td>\n",
              "      <td>0.066865</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019396</td>\n",
              "      <td>0.044407</td>\n",
              "      <td>0.022969</td>\n",
              "      <td>0.006125</td>\n",
              "      <td>0.050532</td>\n",
              "      <td>0.099532</td>\n",
              "      <td>0.043386</td>\n",
              "      <td>0.029604</td>\n",
              "      <td>0.262867</td>\n",
              "      <td>0.137303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>julius-caesar</th>\n",
              "      <td>0.071358</td>\n",
              "      <td>0.038013</td>\n",
              "      <td>0.022675</td>\n",
              "      <td>0.438153</td>\n",
              "      <td>0.072692</td>\n",
              "      <td>0.090698</td>\n",
              "      <td>0.038680</td>\n",
              "      <td>0.100702</td>\n",
              "      <td>0.099368</td>\n",
              "      <td>0.068024</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020007</td>\n",
              "      <td>0.031344</td>\n",
              "      <td>0.022008</td>\n",
              "      <td>0.028677</td>\n",
              "      <td>0.109372</td>\n",
              "      <td>0.104703</td>\n",
              "      <td>0.029344</td>\n",
              "      <td>0.028010</td>\n",
              "      <td>0.268094</td>\n",
              "      <td>0.088698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the-tempest</th>\n",
              "      <td>0.078884</td>\n",
              "      <td>0.033214</td>\n",
              "      <td>0.024911</td>\n",
              "      <td>0.440091</td>\n",
              "      <td>0.063107</td>\n",
              "      <td>0.096322</td>\n",
              "      <td>0.048991</td>\n",
              "      <td>0.117911</td>\n",
              "      <td>0.093831</td>\n",
              "      <td>0.064768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030723</td>\n",
              "      <td>0.061447</td>\n",
              "      <td>0.027402</td>\n",
              "      <td>0.012455</td>\n",
              "      <td>0.067259</td>\n",
              "      <td>0.120402</td>\n",
              "      <td>0.041518</td>\n",
              "      <td>0.022420</td>\n",
              "      <td>0.183510</td>\n",
              "      <td>0.082206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macbeth</th>\n",
              "      <td>0.084312</td>\n",
              "      <td>0.022787</td>\n",
              "      <td>0.023547</td>\n",
              "      <td>0.434473</td>\n",
              "      <td>0.056208</td>\n",
              "      <td>0.067602</td>\n",
              "      <td>0.040257</td>\n",
              "      <td>0.106340</td>\n",
              "      <td>0.093427</td>\n",
              "      <td>0.039498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.060765</td>\n",
              "      <td>0.037219</td>\n",
              "      <td>0.024306</td>\n",
              "      <td>0.055448</td>\n",
              "      <td>0.120012</td>\n",
              "      <td>0.035700</td>\n",
              "      <td>0.043295</td>\n",
              "      <td>0.159509</td>\n",
              "      <td>0.091908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hamlet</th>\n",
              "      <td>0.052672</td>\n",
              "      <td>0.022865</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.397695</td>\n",
              "      <td>0.053080</td>\n",
              "      <td>0.094320</td>\n",
              "      <td>0.034298</td>\n",
              "      <td>0.092278</td>\n",
              "      <td>0.112286</td>\n",
              "      <td>0.049814</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022457</td>\n",
              "      <td>0.026132</td>\n",
              "      <td>0.017557</td>\n",
              "      <td>0.025724</td>\n",
              "      <td>0.070638</td>\n",
              "      <td>0.113511</td>\n",
              "      <td>0.033073</td>\n",
              "      <td>0.017557</td>\n",
              "      <td>0.227838</td>\n",
              "      <td>0.098811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the-taming-of-the-shrew</th>\n",
              "      <td>0.073475</td>\n",
              "      <td>0.048181</td>\n",
              "      <td>0.028306</td>\n",
              "      <td>0.468556</td>\n",
              "      <td>0.051192</td>\n",
              "      <td>0.198143</td>\n",
              "      <td>0.043363</td>\n",
              "      <td>0.123463</td>\n",
              "      <td>0.089134</td>\n",
              "      <td>0.042760</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025295</td>\n",
              "      <td>0.015659</td>\n",
              "      <td>0.011443</td>\n",
              "      <td>0.040953</td>\n",
              "      <td>0.093952</td>\n",
              "      <td>0.118042</td>\n",
              "      <td>0.021079</td>\n",
              "      <td>0.019874</td>\n",
              "      <td>0.313174</td>\n",
              "      <td>0.111418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>coriolanus</th>\n",
              "      <td>0.082333</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.026365</td>\n",
              "      <td>0.344134</td>\n",
              "      <td>0.073082</td>\n",
              "      <td>0.106386</td>\n",
              "      <td>0.038391</td>\n",
              "      <td>0.097597</td>\n",
              "      <td>0.088809</td>\n",
              "      <td>0.051343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024515</td>\n",
              "      <td>0.047642</td>\n",
              "      <td>0.020815</td>\n",
              "      <td>0.016189</td>\n",
              "      <td>0.058743</td>\n",
              "      <td>0.143389</td>\n",
              "      <td>0.045330</td>\n",
              "      <td>0.024977</td>\n",
              "      <td>0.303430</td>\n",
              "      <td>0.138764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>othello</th>\n",
              "      <td>0.050200</td>\n",
              "      <td>0.047158</td>\n",
              "      <td>0.026875</td>\n",
              "      <td>0.403123</td>\n",
              "      <td>0.053243</td>\n",
              "      <td>0.086710</td>\n",
              "      <td>0.034988</td>\n",
              "      <td>0.113077</td>\n",
              "      <td>0.113584</td>\n",
              "      <td>0.059328</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019269</td>\n",
              "      <td>0.023325</td>\n",
              "      <td>0.018255</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.078596</td>\n",
              "      <td>0.114091</td>\n",
              "      <td>0.041073</td>\n",
              "      <td>0.034481</td>\n",
              "      <td>0.251508</td>\n",
              "      <td>0.106485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>romeo-and-juliet</th>\n",
              "      <td>0.058733</td>\n",
              "      <td>0.034213</td>\n",
              "      <td>0.046188</td>\n",
              "      <td>0.410559</td>\n",
              "      <td>0.038205</td>\n",
              "      <td>0.089525</td>\n",
              "      <td>0.037064</td>\n",
              "      <td>0.115755</td>\n",
              "      <td>0.104921</td>\n",
              "      <td>0.067286</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033073</td>\n",
              "      <td>0.035924</td>\n",
              "      <td>0.018817</td>\n",
              "      <td>0.022239</td>\n",
              "      <td>0.084393</td>\n",
              "      <td>0.144836</td>\n",
              "      <td>0.031362</td>\n",
              "      <td>0.026800</td>\n",
              "      <td>0.172777</td>\n",
              "      <td>0.058733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>measure-for-measure</th>\n",
              "      <td>0.050237</td>\n",
              "      <td>0.028544</td>\n",
              "      <td>0.026831</td>\n",
              "      <td>0.331108</td>\n",
              "      <td>0.050237</td>\n",
              "      <td>0.176401</td>\n",
              "      <td>0.030256</td>\n",
              "      <td>0.119313</td>\n",
              "      <td>0.112463</td>\n",
              "      <td>0.066222</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018268</td>\n",
              "      <td>0.033111</td>\n",
              "      <td>0.019410</td>\n",
              "      <td>0.022264</td>\n",
              "      <td>0.062226</td>\n",
              "      <td>0.119884</td>\n",
              "      <td>0.050808</td>\n",
              "      <td>0.029686</td>\n",
              "      <td>0.304277</td>\n",
              "      <td>0.166696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>antony-and-cleopatra</th>\n",
              "      <td>0.071260</td>\n",
              "      <td>0.028949</td>\n",
              "      <td>0.027279</td>\n",
              "      <td>0.378013</td>\n",
              "      <td>0.047321</td>\n",
              "      <td>0.077384</td>\n",
              "      <td>0.045094</td>\n",
              "      <td>0.104107</td>\n",
              "      <td>0.103550</td>\n",
              "      <td>0.058456</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028393</td>\n",
              "      <td>0.060126</td>\n",
              "      <td>0.015588</td>\n",
              "      <td>0.015031</td>\n",
              "      <td>0.070703</td>\n",
              "      <td>0.118581</td>\n",
              "      <td>0.036187</td>\n",
              "      <td>0.025052</td>\n",
              "      <td>0.207100</td>\n",
              "      <td>0.077941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>henry-vi-part-2</th>\n",
              "      <td>0.077166</td>\n",
              "      <td>0.030384</td>\n",
              "      <td>0.029902</td>\n",
              "      <td>0.468783</td>\n",
              "      <td>0.043406</td>\n",
              "      <td>0.083436</td>\n",
              "      <td>0.033760</td>\n",
              "      <td>0.123465</td>\n",
              "      <td>0.084400</td>\n",
              "      <td>0.059804</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017845</td>\n",
              "      <td>0.013022</td>\n",
              "      <td>0.018327</td>\n",
              "      <td>0.020738</td>\n",
              "      <td>0.066556</td>\n",
              "      <td>0.131664</td>\n",
              "      <td>0.019291</td>\n",
              "      <td>0.020256</td>\n",
              "      <td>0.105139</td>\n",
              "      <td>0.080542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>titus-andronicus</th>\n",
              "      <td>0.064351</td>\n",
              "      <td>0.018999</td>\n",
              "      <td>0.025128</td>\n",
              "      <td>0.527681</td>\n",
              "      <td>0.049642</td>\n",
              "      <td>0.072932</td>\n",
              "      <td>0.039224</td>\n",
              "      <td>0.093156</td>\n",
              "      <td>0.074770</td>\n",
              "      <td>0.053933</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016547</td>\n",
              "      <td>0.017160</td>\n",
              "      <td>0.016547</td>\n",
              "      <td>0.027579</td>\n",
              "      <td>0.081512</td>\n",
              "      <td>0.165475</td>\n",
              "      <td>0.030643</td>\n",
              "      <td>0.015322</td>\n",
              "      <td>0.153830</td>\n",
              "      <td>0.075996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>twelfth-night</th>\n",
              "      <td>0.034477</td>\n",
              "      <td>0.056594</td>\n",
              "      <td>0.039031</td>\n",
              "      <td>0.352577</td>\n",
              "      <td>0.054643</td>\n",
              "      <td>0.100179</td>\n",
              "      <td>0.039031</td>\n",
              "      <td>0.130753</td>\n",
              "      <td>0.109286</td>\n",
              "      <td>0.059847</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018214</td>\n",
              "      <td>0.015612</td>\n",
              "      <td>0.014311</td>\n",
              "      <td>0.028622</td>\n",
              "      <td>0.101480</td>\n",
              "      <td>0.111888</td>\n",
              "      <td>0.042283</td>\n",
              "      <td>0.025370</td>\n",
              "      <td>0.297283</td>\n",
              "      <td>0.111888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>henry-iv-part-2</th>\n",
              "      <td>0.049233</td>\n",
              "      <td>0.031416</td>\n",
              "      <td>0.039387</td>\n",
              "      <td>0.425283</td>\n",
              "      <td>0.049233</td>\n",
              "      <td>0.082056</td>\n",
              "      <td>0.037980</td>\n",
              "      <td>0.089558</td>\n",
              "      <td>0.081587</td>\n",
              "      <td>0.053922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017349</td>\n",
              "      <td>0.037042</td>\n",
              "      <td>0.014536</td>\n",
              "      <td>0.012660</td>\n",
              "      <td>0.074085</td>\n",
              "      <td>0.131289</td>\n",
              "      <td>0.035167</td>\n",
              "      <td>0.018287</td>\n",
              "      <td>0.240540</td>\n",
              "      <td>0.113940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>king-lear</th>\n",
              "      <td>0.056417</td>\n",
              "      <td>0.044925</td>\n",
              "      <td>0.024552</td>\n",
              "      <td>0.389696</td>\n",
              "      <td>0.065298</td>\n",
              "      <td>0.064775</td>\n",
              "      <td>0.031865</td>\n",
              "      <td>0.085670</td>\n",
              "      <td>0.067909</td>\n",
              "      <td>0.047537</td>\n",
              "      <td>...</td>\n",
              "      <td>0.029776</td>\n",
              "      <td>0.043358</td>\n",
              "      <td>0.038656</td>\n",
              "      <td>0.026119</td>\n",
              "      <td>0.068432</td>\n",
              "      <td>0.108133</td>\n",
              "      <td>0.032910</td>\n",
              "      <td>0.027164</td>\n",
              "      <td>0.245519</td>\n",
              "      <td>0.116491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the-comedy-of-errors</th>\n",
              "      <td>0.036672</td>\n",
              "      <td>0.044312</td>\n",
              "      <td>0.029796</td>\n",
              "      <td>0.351443</td>\n",
              "      <td>0.033616</td>\n",
              "      <td>0.044312</td>\n",
              "      <td>0.060356</td>\n",
              "      <td>0.064176</td>\n",
              "      <td>0.085569</td>\n",
              "      <td>0.067996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021392</td>\n",
              "      <td>0.016808</td>\n",
              "      <td>0.015280</td>\n",
              "      <td>0.025976</td>\n",
              "      <td>0.067232</td>\n",
              "      <td>0.110017</td>\n",
              "      <td>0.033616</td>\n",
              "      <td>0.015280</td>\n",
              "      <td>0.234550</td>\n",
              "      <td>0.089389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the-two-gentlemen-of-verona</th>\n",
              "      <td>0.049212</td>\n",
              "      <td>0.035371</td>\n",
              "      <td>0.026913</td>\n",
              "      <td>0.338330</td>\n",
              "      <td>0.055363</td>\n",
              "      <td>0.137639</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>0.113802</td>\n",
              "      <td>0.113802</td>\n",
              "      <td>0.063821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023837</td>\n",
              "      <td>0.032295</td>\n",
              "      <td>0.016917</td>\n",
              "      <td>0.049212</td>\n",
              "      <td>0.086120</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.047674</td>\n",
              "      <td>0.032295</td>\n",
              "      <td>0.256055</td>\n",
              "      <td>0.125336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.ipynb_checkpoints</th>\n",
              "      <td>0.056709</td>\n",
              "      <td>0.042254</td>\n",
              "      <td>0.031135</td>\n",
              "      <td>0.344704</td>\n",
              "      <td>0.050594</td>\n",
              "      <td>0.080060</td>\n",
              "      <td>0.037806</td>\n",
              "      <td>0.108415</td>\n",
              "      <td>0.104523</td>\n",
              "      <td>0.053930</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020571</td>\n",
              "      <td>0.048370</td>\n",
              "      <td>0.012787</td>\n",
              "      <td>0.016679</td>\n",
              "      <td>0.087288</td>\n",
              "      <td>0.092848</td>\n",
              "      <td>0.046702</td>\n",
              "      <td>0.027243</td>\n",
              "      <td>0.274095</td>\n",
              "      <td>0.122314</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  all        am        an       and       are  \\\n",
              "much-ado-about-nothing       0.054234  0.046922  0.052406  0.396703  0.056063   \n",
              "richard-iii                  0.064522  0.025982  0.012991  0.403587  0.041138   \n",
              "the-winters-tale             0.054603  0.037802  0.027826  0.350719  0.057753   \n",
              "richard-ii                   0.061009  0.024290  0.020901  0.415197  0.048016   \n",
              "henry-vi-part-3              0.060131  0.023441  0.019364  0.483085  0.037709   \n",
              "the-two-noble-kinsmen        0.080347  0.043826  0.024160  0.472531  0.078100   \n",
              "timon-of-athens              0.097536  0.030527  0.030527  0.400568  0.064031   \n",
              "the-merchant-of-venice       0.050456  0.044589  0.032268  0.361405  0.059256   \n",
              "loves-labors-lost            0.054099  0.033061  0.042077  0.349241  0.060711   \n",
              "troilus-and-cressida         0.075956  0.018612  0.037727  0.412981  0.053320   \n",
              "a-midsummer-nights-dream     0.074013  0.044093  0.025983  0.466910  0.053541   \n",
              "henry-iv-part-1              0.070749  0.035632  0.043379  0.449279  0.041829   \n",
              "henry-vi-part-1              0.050526  0.031504  0.018427  0.454733  0.043987   \n",
              "henry-v                      0.062621  0.011884  0.029254  0.458000  0.044794   \n",
              "pericles                     0.064438  0.028325  0.024076  0.374592  0.063022   \n",
              "the-merry-wives-of-windsor   0.046426  0.045831  0.030951  0.392834  0.039879   \n",
              "as-you-like-it               0.054894  0.042314  0.023444  0.400838  0.063471   \n",
              "king-john                    0.056905  0.024215  0.030269  0.427393  0.032690   \n",
              "cymbeline                    0.058577  0.042602  0.018880  0.353884  0.056641   \n",
              "alls-well-that-ends-well     0.056709  0.042254  0.031135  0.344704  0.050594   \n",
              "henry-viii                   0.082688  0.033688  0.027052  0.379753  0.066865   \n",
              "julius-caesar                0.071358  0.038013  0.022675  0.438153  0.072692   \n",
              "the-tempest                  0.078884  0.033214  0.024911  0.440091  0.063107   \n",
              "macbeth                      0.084312  0.022787  0.023547  0.434473  0.056208   \n",
              "hamlet                       0.052672  0.022865  0.024907  0.397695  0.053080   \n",
              "the-taming-of-the-shrew      0.073475  0.048181  0.028306  0.468556  0.051192   \n",
              "coriolanus                   0.082333  0.021277  0.026365  0.344134  0.073082   \n",
              "othello                      0.050200  0.047158  0.026875  0.403123  0.053243   \n",
              "romeo-and-juliet             0.058733  0.034213  0.046188  0.410559  0.038205   \n",
              "measure-for-measure          0.050237  0.028544  0.026831  0.331108  0.050237   \n",
              "antony-and-cleopatra         0.071260  0.028949  0.027279  0.378013  0.047321   \n",
              "henry-vi-part-2              0.077166  0.030384  0.029902  0.468783  0.043406   \n",
              "titus-andronicus             0.064351  0.018999  0.025128  0.527681  0.049642   \n",
              "twelfth-night                0.034477  0.056594  0.039031  0.352577  0.054643   \n",
              "henry-iv-part-2              0.049233  0.031416  0.039387  0.425283  0.049233   \n",
              "king-lear                    0.056417  0.044925  0.024552  0.389696  0.065298   \n",
              "the-comedy-of-errors         0.036672  0.044312  0.029796  0.351443  0.033616   \n",
              "the-two-gentlemen-of-verona  0.049212  0.035371  0.026913  0.338330  0.055363   \n",
              ".ipynb_checkpoints           0.056709  0.042254  0.031135  0.344704  0.050594   \n",
              "\n",
              "                                   as        at        be       but        by  \\\n",
              "much-ado-about-nothing       0.087141  0.034125  0.110297  0.106031  0.060938   \n",
              "richard-iii                  0.059759  0.042437  0.095700  0.084009  0.073183   \n",
              "the-winters-tale             0.121807  0.031502  0.132307  0.117606  0.067729   \n",
              "richard-ii                   0.080215  0.035588  0.094337  0.084734  0.053665   \n",
              "henry-vi-part-3              0.074399  0.042805  0.105993  0.100388  0.056054   \n",
              "the-two-noble-kinsmen        0.088213  0.037083  0.116307  0.096079  0.057872   \n",
              "timon-of-athens              0.062542  0.037972  0.096047  0.102003  0.047651   \n",
              "the-merchant-of-venice       0.111472  0.036962  0.109712  0.106779  0.062190   \n",
              "loves-labors-lost            0.093772  0.039673  0.084756  0.095575  0.073335   \n",
              "troilus-and-cressida         0.122737  0.043260  0.098592  0.103119  0.055332   \n",
              "a-midsummer-nights-dream     0.115743  0.033857  0.082674  0.096059  0.057478   \n",
              "henry-iv-part-1              0.127554  0.045961  0.100184  0.092438  0.064035   \n",
              "henry-vi-part-1              0.070736  0.041015  0.137312  0.090947  0.060631   \n",
              "henry-v                      0.090503  0.035653  0.080904  0.075419  0.047994   \n",
              "pericles                     0.094179  0.057357  0.097012  0.106217  0.072228   \n",
              "the-merry-wives-of-windsor   0.108922  0.049402  0.126183  0.073805  0.059520   \n",
              "as-you-like-it               0.172686  0.033737  0.108072  0.110931  0.052606   \n",
              "king-john                    0.079909  0.038138  0.104124  0.092017  0.064170   \n",
              "cymbeline                    0.214944  0.039697  0.125384  0.110377  0.063418   \n",
              "alls-well-that-ends-well     0.080060  0.037806  0.108415  0.104523  0.053930   \n",
              "henry-viii                   0.085240  0.042365  0.095449  0.068396  0.066865   \n",
              "julius-caesar                0.090698  0.038680  0.100702  0.099368  0.068024   \n",
              "the-tempest                  0.096322  0.048991  0.117911  0.093831  0.064768   \n",
              "macbeth                      0.067602  0.040257  0.106340  0.093427  0.039498   \n",
              "hamlet                       0.094320  0.034298  0.092278  0.112286  0.049814   \n",
              "the-taming-of-the-shrew      0.198143  0.043363  0.123463  0.089134  0.042760   \n",
              "coriolanus                   0.106386  0.038391  0.097597  0.088809  0.051343   \n",
              "othello                      0.086710  0.034988  0.113077  0.113584  0.059328   \n",
              "romeo-and-juliet             0.089525  0.037064  0.115755  0.104921  0.067286   \n",
              "measure-for-measure          0.176401  0.030256  0.119313  0.112463  0.066222   \n",
              "antony-and-cleopatra         0.077384  0.045094  0.104107  0.103550  0.058456   \n",
              "henry-vi-part-2              0.083436  0.033760  0.123465  0.084400  0.059804   \n",
              "titus-andronicus             0.072932  0.039224  0.093156  0.074770  0.053933   \n",
              "twelfth-night                0.100179  0.039031  0.130753  0.109286  0.059847   \n",
              "henry-iv-part-2              0.082056  0.037980  0.089558  0.081587  0.053922   \n",
              "king-lear                    0.064775  0.031865  0.085670  0.067909  0.047537   \n",
              "the-comedy-of-errors         0.044312  0.060356  0.064176  0.085569  0.067996   \n",
              "the-two-gentlemen-of-verona  0.137639  0.039216  0.113802  0.113802  0.063821   \n",
              ".ipynb_checkpoints           0.080060  0.037806  0.108415  0.104523  0.053930   \n",
              "\n",
              "                             ...     where     which       who       why  \\\n",
              "much-ado-about-nothing       ...  0.007313  0.034125  0.019500  0.029859   \n",
              "richard-iii                  ...  0.020786  0.033344  0.026848  0.019920   \n",
              "the-winters-tale             ...  0.014701  0.063528  0.022051  0.014176   \n",
              "richard-ii                   ...  0.021466  0.040108  0.018642  0.016382   \n",
              "henry-vi-part-3              ...  0.022931  0.019874  0.014778  0.036180   \n",
              "the-two-noble-kinsmen        ...  0.029779  0.037083  0.019104  0.020227   \n",
              "timon-of-athens              ...  0.014146  0.035738  0.023826  0.023826   \n",
              "the-merchant-of-venice       ...  0.016427  0.035788  0.027575  0.023468   \n",
              "loves-labors-lost            ...  0.019235  0.040875  0.015028  0.022241   \n",
              "troilus-and-cressida         ...  0.019618  0.021630  0.024648  0.031690   \n",
              "a-midsummer-nights-dream     ...  0.022834  0.029920  0.009448  0.017322   \n",
              "henry-iv-part-1              ...  0.017558  0.025304  0.015492  0.029436   \n",
              "henry-vi-part-1              ...  0.020210  0.024966  0.013077  0.009511   \n",
              "henry-v                      ...  0.010056  0.031082  0.015998  0.012341   \n",
              "pericles                     ...  0.027616  0.051692  0.046735  0.018411   \n",
              "the-merry-wives-of-windsor   ...  0.012499  0.015475  0.012499  0.029165   \n",
              "as-you-like-it               ...  0.010864  0.033737  0.023444  0.032593   \n",
              "king-john                    ...  0.018767  0.036928  0.022399  0.013924   \n",
              "cymbeline                    ...  0.026142  0.060030  0.028562  0.016460   \n",
              "alls-well-that-ends-well     ...  0.020571  0.048370  0.012787  0.016679   \n",
              "henry-viii                   ...  0.019396  0.044407  0.022969  0.006125   \n",
              "julius-caesar                ...  0.020007  0.031344  0.022008  0.028677   \n",
              "the-tempest                  ...  0.030723  0.061447  0.027402  0.012455   \n",
              "macbeth                      ...  0.027344  0.060765  0.037219  0.024306   \n",
              "hamlet                       ...  0.022457  0.026132  0.017557  0.025724   \n",
              "the-taming-of-the-shrew      ...  0.025295  0.015659  0.011443  0.040953   \n",
              "coriolanus                   ...  0.024515  0.047642  0.020815  0.016189   \n",
              "othello                      ...  0.019269  0.023325  0.018255  0.029917   \n",
              "romeo-and-juliet             ...  0.033073  0.035924  0.018817  0.022239   \n",
              "measure-for-measure          ...  0.018268  0.033111  0.019410  0.022264   \n",
              "antony-and-cleopatra         ...  0.028393  0.060126  0.015588  0.015031   \n",
              "henry-vi-part-2              ...  0.017845  0.013022  0.018327  0.020738   \n",
              "titus-andronicus             ...  0.016547  0.017160  0.016547  0.027579   \n",
              "twelfth-night                ...  0.018214  0.015612  0.014311  0.028622   \n",
              "henry-iv-part-2              ...  0.017349  0.037042  0.014536  0.012660   \n",
              "king-lear                    ...  0.029776  0.043358  0.038656  0.026119   \n",
              "the-comedy-of-errors         ...  0.021392  0.016808  0.015280  0.025976   \n",
              "the-two-gentlemen-of-verona  ...  0.023837  0.032295  0.016917  0.049212   \n",
              ".ipynb_checkpoints           ...  0.020571  0.048370  0.012787  0.016679   \n",
              "\n",
              "                                 will      with     would       yet       you  \\\n",
              "much-ado-about-nothing       0.120047  0.125531  0.053625  0.020719  0.303469   \n",
              "richard-iii                  0.070151  0.119084  0.027281  0.020353  0.169749   \n",
              "the-winters-tale             0.060378  0.109206  0.040952  0.026776  0.247813   \n",
              "richard-ii                   0.054795  0.145743  0.025420  0.028245  0.091513   \n",
              "henry-vi-part-3              0.070322  0.141155  0.024460  0.025989  0.102936   \n",
              "the-two-noble-kinsmen        0.065177  0.111250  0.047197  0.041016  0.247784   \n",
              "timon-of-athens              0.048396  0.127318  0.040206  0.023081  0.214430   \n",
              "the-merchant-of-venice       0.077444  0.114406  0.042242  0.020534  0.265187   \n",
              "loves-labors-lost            0.106395  0.100985  0.031858  0.010820  0.204375   \n",
              "troilus-and-cressida         0.076459  0.115192  0.038230  0.023642  0.228875   \n",
              "a-midsummer-nights-dream     0.090547  0.142514  0.031495  0.021259  0.217314   \n",
              "henry-iv-part-1              0.070232  0.109996  0.029952  0.022722  0.180744   \n",
              "henry-vi-part-1              0.088569  0.161088  0.023777  0.021994  0.120073   \n",
              "henry-v                      0.085018  0.114272  0.029711  0.017826  0.170036   \n",
              "pericles                     0.067979  0.114006  0.040362  0.033281  0.256337   \n",
              "the-merry-wives-of-windsor   0.131540  0.114279  0.044640  0.018451  0.328552   \n",
              "as-you-like-it               0.093205  0.114362  0.041170  0.029734  0.302487   \n",
              "king-john                    0.069013  0.132577  0.026031  0.018161  0.137420   \n",
              "cymbeline                    0.057609  0.106988  0.038245  0.029047  0.207199   \n",
              "alls-well-that-ends-well     0.087288  0.092848  0.046702  0.027243  0.274095   \n",
              "henry-viii                   0.050532  0.099532  0.043386  0.029604  0.262867   \n",
              "julius-caesar                0.109372  0.104703  0.029344  0.028010  0.268094   \n",
              "the-tempest                  0.067259  0.120402  0.041518  0.022420  0.183510   \n",
              "macbeth                      0.055448  0.120012  0.035700  0.043295  0.159509   \n",
              "hamlet                       0.070638  0.113511  0.033073  0.017557  0.227838   \n",
              "the-taming-of-the-shrew      0.093952  0.118042  0.021079  0.019874  0.313174   \n",
              "coriolanus                   0.058743  0.143389  0.045330  0.024977  0.303430   \n",
              "othello                      0.078596  0.114091  0.041073  0.034481  0.251508   \n",
              "romeo-and-juliet             0.084393  0.144836  0.031362  0.026800  0.172777   \n",
              "measure-for-measure          0.062226  0.119884  0.050808  0.029686  0.304277   \n",
              "antony-and-cleopatra         0.070703  0.118581  0.036187  0.025052  0.207100   \n",
              "henry-vi-part-2              0.066556  0.131664  0.019291  0.020256  0.105139   \n",
              "titus-andronicus             0.081512  0.165475  0.030643  0.015322  0.153830   \n",
              "twelfth-night                0.101480  0.111888  0.042283  0.025370  0.297283   \n",
              "henry-iv-part-2              0.074085  0.131289  0.035167  0.018287  0.240540   \n",
              "king-lear                    0.068432  0.108133  0.032910  0.027164  0.245519   \n",
              "the-comedy-of-errors         0.067232  0.110017  0.033616  0.015280  0.234550   \n",
              "the-two-gentlemen-of-verona  0.086120  0.117647  0.047674  0.032295  0.256055   \n",
              ".ipynb_checkpoints           0.087288  0.092848  0.046702  0.027243  0.274095   \n",
              "\n",
              "                                 your  \n",
              "much-ado-about-nothing       0.111516  \n",
              "richard-iii                  0.118651  \n",
              "the-winters-tale             0.138082  \n",
              "richard-ii                   0.071177  \n",
              "henry-vi-part-3              0.072870  \n",
              "the-two-noble-kinsmen        0.103946  \n",
              "timon-of-athens              0.099770  \n",
              "the-merchant-of-venice       0.102672  \n",
              "loves-labors-lost            0.096778  \n",
              "troilus-and-cressida         0.064387  \n",
              "a-midsummer-nights-dream     0.096059  \n",
              "henry-iv-part-1              0.067134  \n",
              "henry-vi-part-1              0.091541  \n",
              "henry-v                      0.105130  \n",
              "pericles                     0.138082  \n",
              "the-merry-wives-of-windsor   0.122612  \n",
              "as-you-like-it               0.110359  \n",
              "king-john                    0.107756  \n",
              "cymbeline                    0.112797  \n",
              "alls-well-that-ends-well     0.122314  \n",
              "henry-viii                   0.137303  \n",
              "julius-caesar                0.088698  \n",
              "the-tempest                  0.082206  \n",
              "macbeth                      0.091908  \n",
              "hamlet                       0.098811  \n",
              "the-taming-of-the-shrew      0.111418  \n",
              "coriolanus                   0.138764  \n",
              "othello                      0.106485  \n",
              "romeo-and-juliet             0.058733  \n",
              "measure-for-measure          0.166696  \n",
              "antony-and-cleopatra         0.077941  \n",
              "henry-vi-part-2              0.080542  \n",
              "titus-andronicus             0.075996  \n",
              "twelfth-night                0.111888  \n",
              "henry-iv-part-2              0.113940  \n",
              "king-lear                    0.116491  \n",
              "the-comedy-of-errors         0.089389  \n",
              "the-two-gentlemen-of-verona  0.125336  \n",
              ".ipynb_checkpoints           0.122314  \n",
              "\n",
              "[39 rows x 100 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shakespeare = pd.DataFrame(shakespeare.toarray(), index=titles, columns=vectorizer.get_feature_names_out())\n",
        "shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XaQgtedxX15"
      },
      "source": [
        "You just created data. These are normalized counts of the most frequent 100 words in Shakespeare's plays.\n",
        "\n",
        "In this penultimate step, do these things:\n",
        "\n",
        "1.   Choose a number of clusters you think you will need, and explain why you chose that number for the value of K. For a model, I have made available the notebook we went over in class when we first discussed the method (Remember, this notebook erroneously uses the K-means method on lableled data).  \n",
        "\n",
        "Your answer here\n",
        "\n",
        "2.   Use K-means clustering to classify instances in the Shakespeare data. Do your results match the general way that the Folger Shakespeare Library and literary scholars describe and classify each play? Click on each play, [here](https://www.folger.edu/explore/shakespeares-works/all-works/), for that information.\n",
        "\n",
        "Your answer here\n",
        "\n",
        "\n",
        "1.   Bonus: Create a visualization of your clustering model. Which features seem to be separating the clusters? Suggest 1-2 next steps.\n",
        "\n",
        "Your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/O0lEQVR4nO3deVwV9eL/8fc5rG7ggrIoGrmSKCJuuHTLCsMll1JSE80228ys7s26D7XlZvnLyjLNTE3N3JfcjdzXEhNFpcQtXFBzY1NBYX5/eD3fS4oCAXPgvJ6PxzweMQvzZh5T593MZ+ZYDMMwBAAAYBKr2QEAAIBjo4wAAABTUUYAAICpKCMAAMBUlBEAAGAqyggAADAVZQQAAJiKMgIAAEzlbHaAvMjOztbJkydVoUIFWSwWs+MAAIA8MAxDqamp8vPzk9Wa+/WPElFGTp48KX9/f7NjAACAAjh27Jhq1KiR6/ISUUYqVKgg6fof4+HhYXIaAACQFykpKfL397d9juemRJSRG7dmPDw8KCMAAJQwdxpiwQBWAABgKsoIAAAwFWUEAACYijICAABMRRkBAACmoowAAABTUUYAAICpKCMAAMBUlBEAAGAqyggAADAVZQQAAJiKMgIAAEzl0GVk9t7ZenrJ0/rlxC9mRwEAwGE5dBlZEL9Ak3dN1vqj682OAgCAw3LoMtLMt5kkKeZkjMlJAABwXI5dRvwoIwAAmM2hy0hT36aSpCMXj+jcpXMmpwEAwDE5dBmpVKaS6lSuI0nambTT5DQAADgmhy4jErdqAAAwG2WEQawAAJiKMsKVEQAATOXwZaSpb1NZZNGxlGM6nXba7DgAADgchy8jFdwqqIFXA0kMYgUAwAwOX0YkbtUAAGAmyogoIwAAmIkyIsoIAABmooxIauLTRFaLVUlpSTqZetLsOAAAOBTKiKSyLmXVsGpDSVwdAQCguFFG/otbNQAAmIMy8l83ysiOkztMTgIAgGOhjPzX/14ZMQzD5DQAADgOysh/NfZuLGers85eOqvE5ESz4wAA4DAoI//l7uyuRtUaSWLcCAAAxYky8j8YxAoAQPGjjPwPWxlJoowAAFBcKCP/g0GsAAAUv3yXkY0bN6pLly7y8/OTxWLR4sWL87ztli1b5OzsrCZNmuR3t8UiqFqQXJ1cdfHKRR2+cNjsOAAAOIR8l5H09HQFBwdr3Lhx+douOTlZUVFReuCBB/K7y2Lj6uSqYO9gSYwbAQCguOS7jEREROj9999Xjx498rXdc889pz59+igsLCy/uyxWzf2aS6KMAABQXIplzMjUqVN16NAhjRgxojh297cwiBUAgOLlXNQ7SEhI0JtvvqlNmzbJ2Tlvu8vIyFBGRobt55SUlKKKd5MbZWTnyZ3KNrJltTDGFwCAolSkn7RZWVnq06eP3nnnHdWrVy/P240aNUqenp62yd/fvwhT5hRYNVBlnMsoNTNVCecSim2/AAA4qiItI6mpqYqJidFLL70kZ2dnOTs7691339Xu3bvl7OystWvX3nK7YcOGKTk52TYdO3asKGPm4Gx1VohviCTGjQAAUByK9DaNh4eH4uLicswbP3681q5dq/nz5ysgIOCW27m5ucnNza0oo91WM99m2npsq2JOxqhv476m5QAAwBHku4ykpaXp4MGDtp+PHDmi2NhYVa5cWTVr1tSwYcN04sQJTZ8+XVarVUFBQTm2r1atmtzd3W+ab08YxAoAQPHJdxmJiYnR/fffb/t56NChkqT+/fvr22+/VVJSkhITS/a33t4oI78m/apr2dfkbC3ycb4AADgsi1EC3nuekpIiT09PJScny8PDo8j3l5WdpYofVVRaZprino9TUDX7vYoDAIC9yuvnN8+t3oKT1UlNfZtKYhArAABFjTKSi2a+//eleQAAoOhQRnLxv9/gCwAAig5lJBc3ykjsqVhdzbpqchoAAEovykgualeuLU83T2VkZWjfn/vMjgMAQKlFGcmF1WJVqF+oJG7VAABQlCgjt8EgVgAAih5l5DaaV28uiTICAEBRoozcxo1BrHtO71HGtQyT0wAAUDpRRm6jlmctVSlTRVezryruTNydNwAAAPlGGbkNi8XC+0YAAChilJE7oIwAAFC0KCN3QBkBAKBoUUbu4EYZ2Xtmry5fvWxyGgAASh/KyB1Ur1Bd3uW8lWVkaffp3WbHAQCg1KGM3MH/DmLdcWKHyWkAACh9KCN5YBs3ksS4EQAAChtlJA8YxAoAQNGhjORBqO/1L8yL/zNeaZlpJqcBAKB0oYzkgW8FX1WvUF2GDO1K2mV2HAAAShXKSB5xqwYAgKJBGckjBrECAFA0KCN5xJURAACKBmUkj26UkQPnDij5SrLJaQAAKD0oI3nkVdZLd1W8S5L0a9Kv5oYBAKAUoYzkA7dqAAAofJSRfGjmyyBWAAAKG2UkH7gyAgBA4aOM5ENT36aSpMMXDuv85fMmpwEAoHSgjORDpTKVVKdyHUnSzpM7TU4DAEDpQBnJJ27VAABQuCgj+cQgVgAAChdlJJ9uXBnZcWKHyUkAACgdKCP5FOIbIossOpZyTKfTTpsdBwCAEo8ykk8ebh6q71VfkrQziUGsAAD8XZSRAmAQKwAAhYcyUgC2QayUEQAA/rZ8l5GNGzeqS5cu8vPzk8Vi0eLFi2+7/sKFC/XQQw+patWq8vDwUFhYmFavXl3QvHaBKyMAABSefJeR9PR0BQcHa9y4cXlaf+PGjXrooYe0YsUK7dy5U/fff7+6dOmiXbt25TusvWji00RWi1VJaUk6mXrS7DgAAJRozvndICIiQhEREXle/7PPPsvx8wcffKAffvhBS5cuVUhISH53bxfKuZZTw6oNFXcmTjEnY/RI/UfMjgQAQIlV7GNGsrOzlZqaqsqVK+e6TkZGhlJSUnJM9oZbNQAAFI5iLyNjxoxRenq6evXqles6o0aNkqenp23y9/cvxoR5QxkBAKBwFGsZmTVrlkaOHKk5c+aoWrVqua43bNgwJScn26Zjx44VY8q8uVFGth/frguXL5icBgCAkqvYysicOXP01FNPae7cuXrwwQdvu66bm5s8PDxyTPamiU8TBVQM0IUrF9R3YV9lZWeZHQkAgBKpWMrIrFmzNGDAAH3//ffq1KlTceyyyLk6uWpBrwVyd3bXyoMrNXL9SLMjAQBQIuW7jKSlpSk2NlaxsbGSpCNHjig2NlaJiYmSrt9iiYqKsq0/a9YsRUVFacyYMWrVqpVOnTqlU6dOKTk5uXD+AhOF+Ibo685fS5Le3/S+Fv+22NxAAACUQPkuIzExMQoJCbE9ljt06FCFhIRo+PDhkqSkpCRbMZGkiRMn6tq1a3rxxRfl6+trm1555ZVC+hPM1S+4nwa3GCxJiloUpd/O/mZyIgAAShaLYRiG2SHuJCUlRZ6enkpOTrbL8SNXs67qwRkPauMfG1W/Sn398swv8nCzv5wAABSnvH5+8900hcDFyUVzH5urGh419Pu53xW1KErZRrbZsQAAKBEoI4XEu7y3FvRaIFcnV/3w+w/6YNMHZkcCAKBEoIwUohbVW2hCpwmSpOHrhmtFwgqTEwEAYP8oI4VsYMhADQodJEOG+izoo4PnD5odCQAAu0YZKQJjI8aqtX9rJWckq/uc7krLTDM7EgAAdosyUgRcnVw1r+c8+ZT30d4ze/XUkqdUAh5aAgDAFJSRIuJXwU/ze86Xi9VFc/fN1cdbPzY7EgAAdokyUoTa1GyjsQ+PlSS9ueZNRR+KNjkRAAD2hzJSxAY1G6SBTQYq28jW4wse15ELR8yOBACAXaGMFDGLxaIvO32p5n7Ndf7yefWY20OXrl4yOxYAAHaDMlIM3J3dtaDXAlUtW1Wxp2L13LLnGNAKAMB/UUaKib+nv+b1nCcni5O+2/OdvvjlC7MjAQBgFygjxegfd/1DY8LHSJKGrh6qDUc3mJwIAADzUUaK2eCWg9W3UV9lGVnqOa+njiUfMzsSAACmoowUM4vFoq+7fK0mPk3056U/9eQPTzJ+BADg0CgjJijrUlbzes5TGecyWnNkjSb9OsnsSAAAmIYyYpI6leto1AOjJEmv/fia/rj4h8mJAAAwB2XERC+3fFlta7ZVWmaanl76NLdrAAAOiTJiIqvFqimPTJG7s7t+OvyTvvn1G7MjAQBQ7CgjJqtbpa4+aP+BpOu3axKTE01OBABA8aKM2IHBLQertX9rpWam6pmlz3C7BgDgUCgjdsDJ6mS7XfPjoR81ZdcUsyMBAFBsKCN2or5Xfb1///uSpKE/DuVlaAAAh0EZsSNDWg1RWI0wpWSk6Nllz3K7BgDgECgjdsTJ6qSpXafKzclNqw6u0tTYqWZHAgCgyFFG7Ex9r/p6v/312zWvrn5Vx1OOm5wIAICiRRmxQ6+2elWtarS6frtmKbdrAAClG2XEDt14usbNyU0rD67UtN3TzI4EAECRoYzYqcCqgXr3/nclSUNWDdGJlBMmJwIAoGhQRuzY0LChalG9hZIzkvXcsue4XQMAKJUoI3bM2eqsqV2nytXJVcsTlmvGnhlmRwIAoNBRRuzcPVXv0bv3Xb9d88qqV3Qy9aTJiQAAKFyUkRLgtdavqblfc128cpGnawAApQ5lpAT46+2a7/Z8Z3YkAAAKDWWkhGhYraFG/mOkJGnwqsFKSk0yNxAAAIWEMlKCvNHmDYX6hurilYs8XQMAKDUoIyWIs9VZ33b7Vi5WFy09sFTfx31vdiQAAP62fJeRjRs3qkuXLvLz85PFYtHixYvvuM2GDRsUGhoqd3d33X333frqq68KkhWSgqoFacQ/RkiSXlzxon4+/rPJiQAA+HvyXUbS09MVHByscePG5Wn9I0eOqGPHjmrXrp127dqlt956S4MHD9aCBQvyHRbX/bPNP9W2ZlslZyTrgekP6KfDP5kdCQCAArMYf2PggcVi0aJFi9StW7dc1/nXv/6lJUuWKD4+3jZv0KBB2r17t7Zt25an/aSkpMjT01PJycny8PAoaNxSJS0zTT3m9FD04Wi5Orlq1qOz1COwh9mxAACwyevnd5GPGdm2bZvCw8NzzOvQoYNiYmJ09erVW26TkZGhlJSUHBNyKu9aXkt7L9Vj9zymzKxM9ZzXU1N2TTE7FgAA+VbkZeTUqVPy9vbOMc/b21vXrl3T2bNnb7nNqFGj5OnpaZv8/f2LOmaJ5ObsptmPztZTIU8p28jWU0ue0pitY8yOBQBAvhTL0zQWiyXHzzfuDP11/g3Dhg1TcnKybTp27FiRZyypnKxOmtRlkt5o/YYk6fXo1/X2mrd57BcAUGI4F/UOfHx8dOrUqRzzzpw5I2dnZ1WpUuWW27i5ucnNza2oo5UaFotFox8arcplKmvYmmH6YPMHOn/5vMZ1HCcnq5PZ8QAAuK0ivzISFham6OjoHPN+/PFHNWvWTC4uLkW9e4fyZts39VWnr2SRRV/t/EpPLHpCmVmZZscCAOC28l1G0tLSFBsbq9jYWEnXH92NjY1VYmKipOu3WKKiomzrDxo0SH/88YeGDh2q+Ph4TZkyRZMnT9brr79eOH8Bcniu2XOa9egsuVhdNHvvbHWb3U2Xrl4yOxYAALnKdxmJiYlRSEiIQkJCJElDhw5VSEiIhg8fLklKSkqyFRNJCggI0IoVK7R+/Xo1adJE7733nj7//HM9+uijhfQn4K8igyK1pPcSlXEuo5UHVyp8RrguXrlodiwAAG7pb71npLjwnpGC2ZK4RZ2+76TkjGQFewdr9ROr5V3e+84bAgBQCOzmPSMwT5uabbRhwAZ5l/PW7tO71XZqWx29eNTsWAAA5EAZKeWCfYK1eeBm3VXxLh08f1Btp7TV/j/3mx0LAAAbyogDqFO5jjY/uVn3VL1HJ1JPqN3UdtpxYofZsQAAkEQZcRjVPapr44CNalG9hc5fPq/209tr+YHlZscCAIAy4kiqlK2in/r9pPYB7ZWWmabOszrr5RUv6/LVy2ZHAwA4MMqIg6ngVkEr+qzQKy1fkSSN2zFOoV+HKvZUrLnBAAAOizLigNyc3fTZw59pVd9V8invo/iz8WoxqYX+35b/p2wj2+x4AAAHQxlxYB3qdFDc83Hq1qCbrmZf1T9/+qcenP6gjiXzxYQAgOJDGXFwXmW9tLDXQk3qMkllXcpq3dF1avxVY83dN9fsaAAAB0EZgSwWi55u+rRin4tVc7/munjloiLnRypqUZRSMlLMjgcAKOUoI7CpW6Wutgzcon+3+7esFqtm7JmhJl810ZbELWZHAwCUYpQR5ODi5KL32r+nDQM26K6Kd+nIxSO699t7NXzdcF3Nump2PABAKUQZwS21rdlWsc/Fql/jfso2svXexvfUdmpbHTx/0OxoAIBShjKCXHm6e2p69+ma/ehsVXSvqF9O/KImXzXRN79+oxLwZc8AgBKCMoI7igyK1J5Be3TfXfcp/Wq6nln6jHrO66mMaxlmRwMAlAKUEeSJv6e/1kSt0egHR8vF6qIF8Qv0/PLnuUICAPjbKCPIM6vFqjfavKGlvZfKarFqauxUfbr9U7NjAQBKOMoI8q1DnQ76JPwTSdIb0W9oZcJKkxMBAEoyyggKZHDLwXo65GllG9l6fMHjiv8z3uxIAIASijKCArFYLPqy05dqV7OdUjJS9MjsR3T+8nmzYwEASiDKCArM1clVC3otUC3PWjp4/qB6zevFi9EAAPlGGcHfUrVcVS3pvUTlXMppzZE1Grp6qNmRAAAlDGUEf1tj78aa2WOmLLJo3I5xmhgz0exIAIAShDKCQtG1QVe93/59SdJLK1/S+qPrzQ0EACgxKCMoNMPaDlPvoN66ln1Nj819TIcvHDY7EgCgBKCMoNBYLBZNfmSymvs117nL5/TIrEeUkpFidiwAgJ2jjKBQlXEpo8WPL5ZfBT/t+3Of+i7sq6zsLLNjAQDsGGUEhc6vgp8WRy6Wu7O7lh1YprfXvm12JACAHaOMoEg0r95cUx6ZIkn6aMtH+m7PdyYnAgDYK8oIikzvRr31Vtu3JElPL3laPx//2eREAAB7RBlBkXqv/XvqWr+rMrIy1G1ONx1POW52JACAnaGMoEhZLVZ91+M7NarWSKfSTqnr7K66dPWS2bEAAHaEMoIiV961vJb0XiKvsl76NelXPfnDkzIMw+xYAAA7QRlBsbir4l1a2GuhXKwumrtvrtpPb6//bPyPNhzdoMtXL5sdDwBgIotRAv4XNSUlRZ6enkpOTpaHh4fZcfA3TNk1RU8veVqG/u+0c7G6qJlfM7Wt2VZta7ZVG/82qlK2iokpAQCFIa+f35QRFLv4P+O15sgabUrcpE1/bFJSWtJN6wR6BapdzXa2gnJXxbtksVhMSAsAKCjKCEoEwzB09OJRbUrcpM2Jm7U5cbPiz8bftJ5fBT9bOXnw7gfVwKuBCWkBAPmR18/vAo0ZGT9+vAICAuTu7q7Q0FBt2rTptuvPnDlTwcHBKlu2rHx9ffXkk0/q3LlzBdk1ShmLxaKASgGKCo7S112+1v4X9+vPN/7U4sjFej3sdbWq0UrOVmedTD2pOfvm6OWVLyvwy0ANXzec18wDQCmR7ysjc+bMUb9+/TR+/Hi1adNGEydO1DfffKP9+/erZs2aN62/efNm/eMf/9Cnn36qLl266MSJExo0aJDq1q2rRYsW5WmfXBlxbJeuXtIvJ37R5sTNWnd0ndYeWStJerjOw5rZY6Yql6lsckIAwK0U2W2ali1bqmnTppowYYJtXmBgoLp166ZRo0bdtP7HH3+sCRMm6NChQ7Z5X3zxhUaPHq1jx47laZ+UEfyvGbtn6Nllz+rKtSsKqBighZEL1cSnidmxAAB/USS3aTIzM7Vz506Fh4fnmB8eHq6tW7fecpvWrVvr+PHjWrFihQzD0OnTpzV//nx16tQp1/1kZGQoJSUlxwTc0C+4n7Y9tU0BFQN05OIRhU0O04zdM8yOBQAooHyVkbNnzyorK0ve3t455nt7e+vUqVO33KZ169aaOXOmIiMj5erqKh8fH1WsWFFffPFFrvsZNWqUPD09bZO/v39+YsIBNPFpophnYxRRJ0JXrl1R1OIovbTiJWVmZZodDQCQTwUawPrXRywNw8j1scv9+/dr8ODBGj58uHbu3KlVq1bpyJEjGjRoUK6/f9iwYUpOTrZNeb2dA8dSuUxlLeuzTMPvHS5J+nLHl7p/2v06mXrS5GQAgPxwzs/KXl5ecnJyuukqyJkzZ266WnLDqFGj1KZNG73xxhuSpMaNG6tcuXJq166d3n//ffn6+t60jZubm9zc3PITDQ7KarHqnfvfUfPqzfXEwie09dhWNZ3YVHN7ztW9te41Ox4AIA/ydWXE1dVVoaGhio6OzjE/OjparVu3vuU2ly5dktWaczdOTk6SxPeToNB0rtdZMc/GKKhakE6nn1b7ae312fbPOMcAoATI922aoUOH6ptvvtGUKVMUHx+vV199VYmJibbbLsOGDVNUVJRt/S5dumjhwoWaMGGCDh8+rC1btmjw4MFq0aKF/Pz8Cu8vgcOrU7mOtj+1Xb2DeivLyNKrq19V34V9lZ6ZbnY0AMBt5Os2jSRFRkbq3Llzevfdd5WUlKSgoCCtWLFCtWrVkiQlJSUpMTHRtv6AAQOUmpqqcePG6bXXXlPFihXVvn17ffTRR4X3VwD/Vc61nGb2mKmW1Vvq9ejXNWvvLMWdidOiyEWqU7mO2fEAALfA6+BRam36Y5N6zuup0+mn5enmqe96fKfO9TqbHQsAHEaRvg4eKAna1WqnX5/7Va39Wys5I1ldZnXhNfIAYIcoIyjV/Cr4aV3/dXqx+YuSpPc2vqc+C/so28g2ORkA4AbKCEo9VydXjes4TtO7TZerk6vm7purEetGmB0LAPBflBE4jH7B/TSpyyRJ0vub3tesuFkmJwIASJQROJio4Cj9s/U/JUkDlwzUjhM7TE4EAKCMwOF88MAH6lyvs65cu6Kus7vqRMoJsyMBgEOjjMDhOFmdNLPHTDWs2lBJaUnqNqebLl29ZHYsAHBYlBE4JA83Dy3tvVRVylRRzMkYDfxhIK+OBwCTUEbgsAIqBWhh5EI5W501Z98cvb/xfbMjAYBDoozAod1b615N6DRBkjR8/XAt2L/A5EQA4HgoI3B4Tzd9Wq+0fEWSFLU4SruSdpmcCAAcC2UEkPRx+MfqULuDLl29pEdmP6JTaafMjgQADoMyAkhytjpr9mOzVb9KfR1POa7uc7rryrUrZscCAIdAGQH+q6J7RS3tvVSV3Ctp+/HtenbpszxhAwDFgDIC/I+6VepqXs95crI4acaeGRq9ZbTZkQCg1KOMAH/xwN0P6POIzyVJw9YM05Lfl5icCABKN8oIcAsvNH9Bzzd7XoYM9V3YV3Gn48yOBAClFmUEyMXYh8eqfUB7pWWm6ZHZj+jP9D/NjgQApRJlBMiFi5OL5vWcp9qVauvoxaPqMbeHMrMyzY4FAKUOZQS4jcplKmtp76XycPPQ5sTNen7Z8zxhAwCFjDIC3EFg1UDNfnS2rBarpsROUcfvO+rg+YNmxwKAUoMyAuRBRN0IfdXpK7k6uWrVwVUKGh+kd9a/w4vRAKAQUEaAPHom9BntfX6vHrr7IWVkZWjkhpEKGh+k1QdXmx0NAEo0ygiQD3Wr1NXqJ1Zr7mNz5VfBT4cuHNLDMx9Wz3k9dTzluNnxAKBEoowA+WSxWNSzYU/FvxivV1u9KieLk+bvn6/ALwP1ybZPdDXrqtkRAaBEoYwABeTh5qFPOnyinc/uVFiNMKVlpum1H19T6Neh2py42ex4AFBiUEaAvynYJ1ibB27W5Ecmq0qZKoo7E6d2U9tp4A8DeVEaAOQBZQQoBFaLVQNDBur3l37X0yFPS5Kmxk5V/XH19fXOr5VtZJucEADsF2UEKERVylbRpEcmaevArQr2DtaFKxf03LLn1Hpya+1K2mV2PACwS5QRoAiE+Ycp5tkYfdrhU5V3La+fT/ysZpOa6fllz+tEygmz4wGAXaGMAEXE2eqsIa2G6LcXf1Nkw0hlG9n6audXqv15bQ1ZNUSn0k6ZHREA7AJlBChi1T2qa/Zjs7Wu/zq1rdlWGVkZGvvzWN099m69/uPrOpN+xuyIAGAqyghQTO676z5tHLBRPz7xo1rVaKXL1y5rzLYxChgboDd/elNnL501OyIAmIIyAhQji8Wih2o/pK0Dt2pl35Vq7tdcl65e0kdbPlLA2AD9e+2/df7yebNjAkCxshgl4PvQU1JS5OnpqeTkZHl4eJgdByg0hmFoecJyDV83XLtOXX/axsPNQ6+2elVDWg1RRfeK5gYEgL8hr5/flBHADhiGoR9+/0Ej1o/QntN7JEkV3SvqtbDXNLjlYHm4cd4DKHkoI0AJlG1ka2H8Qo1YP0L7/9wvSapcprLeaP2GXmrxksq7ljc5IQDkXV4/vws0ZmT8+PEKCAiQu7u7QkNDtWnTptuun5GRobffflu1atWSm5ubateurSlTphRk10CpZrVY9dg9j2nPoD2a9egs1a9SX+cvn9ewNcMUMDZAM3bPUAn4/wcAyJd8l5E5c+ZoyJAhevvtt7Vr1y61a9dOERERSkxMzHWbXr16ac2aNZo8ebJ+//13zZo1Sw0aNPhbwYHSzMnqpMeDHte+F/bpu+7fqW7lujp76ayiFkfp0bmP8jgwgFIl37dpWrZsqaZNm2rChAm2eYGBgerWrZtGjRp10/qrVq3S448/rsOHD6ty5coFCsltGji6a9nXNHrLaI1cP1JXs6/Kq6yXJnaeqB6BPcyOBgC5KpLbNJmZmdq5c6fCw8NzzA8PD9fWrVtvuc2SJUvUrFkzjR49WtWrV1e9evX0+uuv6/Lly7nuJyMjQykpKTkmwJE5W531Vru3tOOZHWrs3VhnL53Vo3Mf1RMLn9CFyxfMjgcAf0u+ysjZs2eVlZUlb2/vHPO9vb116tStX219+PBhbd68WXv37tWiRYv02Wefaf78+XrxxRdz3c+oUaPk6elpm/z9/fMTEyi1gn2CteOZHXq73duyWqyaGTdTQROCtDJhpdnRAKDACjSA1WKx5PjZMIyb5t2QnZ0ti8WimTNnqkWLFurYsaM++eQTffvtt7leHRk2bJiSk5Nt07FjxwoSEyiVXJ1c9X7797V14FbVr1JfJ1NPquP3HfXs0meVmpFqdjwAyLd8lREvLy85OTnddBXkzJkzN10tucHX11fVq1eXp6enbV5gYKAMw9Dx48dvuY2bm5s8PDxyTAByalmjpXY9t0tDWg6RJE36dZIaf9VY64+uNzUXAORXvsqIq6urQkNDFR0dnWN+dHS0Wrdufctt2rRpo5MnTyotLc0278CBA7JarapRo0YBIgO4oYxLGX368Kda13+d7qp4l45ePKr7p92vIauG6NLVS2bHA4A8yfdtmqFDh+qbb77RlClTFB8fr1dffVWJiYkaNGiQpOu3WKKiomzr9+nTR1WqVNGTTz6p/fv3a+PGjXrjjTc0cOBAlSlTpvD+EsCB3XfXfdozaI+ebfqsJGnsz2MVMjFE249vNzkZANxZvstIZGSkPvvsM7377rtq0qSJNm7cqBUrVqhWrVqSpKSkpBzvHClfvryio6N18eJFNWvWTH379lWXLl30+eefF95fAUAV3CpoYpeJWtl3pfwq+OnAuQNqM6WN3lrzljKuZZgdDwByxevggVLowuULGrxqsL7b850kqVG1Rprefbqa+DQxNxgAh1Kkr4MHYN8qlamkGd1naEGvBapatqrizsSp+aTmGrl+pDKzMs2OBwA5UEaAUqxHYA/tfWGvegT20LXsa3pnwztqPqm5fk361exoAGBDGQFKuWrlqml+z/ma89gceZX10p7Te9RiUgv9e+2/GUsCwC5QRgAHYLFY1KthL+1/Yb96NeylLCNL/9n0H4V+HaodJ3aYHQ+Ag6OMAA6karmqmvPYHM3vOV/VylXTvj/3qdXkVnrzpzd15doVs+MBcFCUEcABPXrPo9r3wj71adRH2Ua2PtrykUImhmjbsW1mRwPggCgjgIPyKuulmT1manHkYvmU99FvZ39Tmylt9PqPr+vy1dy/VRsAChtlBHBwXRt01b4X9ikqOEqGDI3ZNkbBXwVrc+Jms6MBcBCUEQCqXKaypnWbpmW9l6l6hepKOJ+ge6feqyGrhig9M93seABKOcoIAJtO9Tpp7wt7NbDJQBkyNPbnsWr8VWNtOLrB7GgASjHKCIAcKrpX1OSuk7Wq7yr5e/jr8IXDum/afXryhyd1MvWk2fEAlEKUEQC31KFOB+19Ya/tm4C/jf1Wdb+oq3c3vKtLVy+ZnA5AaUIZAZArDzcPTewyUduf2q7W/q116eoljVg/QvW+qKcZu2co28g2OyKAUoAyAuCOWtZoqc1Pbtacx+borop36UTqCUUtjlLLb1pq0x+bzI4HoISjjADIkxuvlI9/MV4fPvChKrhWUMzJGN377b16bO5jOnT+kNkRAZRQlBEA+eLu7K5/tf2XDg4+qEGhg2S1WLUgfoHuGX+P3vjxDV28ctHsiABKGMoIgAKpVq6aJnSeoN2Ddiu8drgyszL18baPVefzOvryly91Lfua2REBlBCUEQB/S1C1IK3qu0or+qxQoFegzl0+p5dWvqTGExprZcJKGYZhdkQAdo4yAuBvs1gsiqgboT3P79GXHb+UV1kvxZ+NV8fvO+rhmQ9r75m9ZkcEYMcoIwAKjbPVWS80f0EJLyfojdZvyNXJVT8e+lEhE0P077X/Vsa1DLMjArBDlBEAha6ie0WNfmi09r+wX90bdNe17Gv6z6b/KGRiiLYf3252PAB2hjICoMjUrlxbCyMXakGvBfIu5634s/FqPbm1hq4eyltcAdhQRgAUuR6BPbT/xf2KCo6SIUOfbv9UjSY00roj68yOBsAOUEYAFIvKZSprWrdpWtFnhWp41NDhC4fVfnp7DVo2SCkZKWbHA2AiygiAYhVRN0L7XtinQaGDJEkTd05Uw/ENtSJhhcnJAJiFMgKg2Hm4eWhC5wla13+daleqreMpx9Xp+06KWhSlc5fOmR0PQDGjjAAwzX133ac9z+/R0FZDZbVYNWPPDN0z/h7N3z/f7GgAihFlBICpyrqU1ZgOY7Rl4BYFegXqTPoZ9ZzXU4/OfVSn0k6ZHQ9AMaCMALALrWq00q7ndunf7f4tZ6uzFsYv1D1f3qPpu6fzSnmglKOMALAbbs5ueq/9e9rxzA6F+ITowpUL6r+4v1p+01Jz9s7hy/eAUooyAsDuNPFpop+f/lkftP9A7s7u2nFyhx5f8LjqfF5HY7ePVWpGqtkRARQii1ECrn+mpKTI09NTycnJ8vDwMDsOgGL0Z/qfGr9jvMbtGKezl85Kuv66+UGhg/Ryy5flV8HP5IQAcpPXz2/KCIAS4fLVy5q+e7rGbBujhPMJkiQXq4v6NOqj18JeUyPvRiYnBPBXlBEApVK2ka1lB5bp460fa1PiJtv88Nrhej3sdT1494OyWCwmJgRwA2UEQKn3y4lfNGbbGM3fP1/ZRrYkqbF3Y70e9roigyLl6uRqckLAsVFGADiMIxeO6LPtn2nyrslKv5ouSfKr4KdXWr6iZ0OfVUX3iuYGBBwUZQSAw7lw+YIm7pyoz3/+XElpSZKuv3p+WNtheqXlKyrjUsbkhIBjyevnd4Ee7R0/frwCAgLk7u6u0NBQbdq06c4bSdqyZYucnZ3VpEmTguwWAG6rUplKerPtmzryyhF92/VbBVULUkpGioatGab64+prxu4Ztts5AOxHvsvInDlzNGTIEL399tvatWuX2rVrp4iICCUmJt52u+TkZEVFRemBBx4ocFgAyAs3Zzf1b9Jfuwft1ozuM+Tv4a9jKccUtThKzb5uprVH1podEcD/yPdtmpYtW6pp06aaMGGCbV5gYKC6deumUaNG5brd448/rrp168rJyUmLFy9WbGxsnvfJbRoAf8flq5f1+c+f64PNHyglI0WS1KluJ41+aLTuqXqPyemA0qtIbtNkZmZq586dCg8PzzE/PDxcW7duzXW7qVOn6tChQxoxYkSe9pORkaGUlJQcEwAUVBmXMvpX23/p4MsH9XKLl+VsddbyhOVqNKGRnlv6HF/IB5gsX2Xk7NmzysrKkre3d4753t7eOnXq1v8yJyQk6M0339TMmTPl7Oycp/2MGjVKnp6etsnf3z8/MQHglqqWq6rPIz7Xvhf2qXuD7so2svX1r1+rzud19O6Gd5WemW52RMAhFWgA619fKGQYxi1fMpSVlaU+ffronXfeUb169fL8+4cNG6bk5GTbdOzYsYLEBIBbqlelnhZGLtSmJzepRfUWSr+arhHrR6juF3U1+dfJysrOMjsi4FDyVUa8vLzk5OR001WQM2fO3HS1RJJSU1MVExOjl156Sc7OznJ2dta7776r3bt3y9nZWWvX3noQmZubmzw8PHJMAFDY2tZsq+1PbdfsR2croGKAktKS9PTSpxUyMUSrD642Ox7gMPJVRlxdXRUaGqro6Ogc86Ojo9W6deub1vfw8FBcXJxiY2Nt06BBg1S/fn3FxsaqZcuWfy89APxNFotFkUGRin8xXmPCx6iie0XFnYnTwzMfVviMcG36Y5NKwOuYgBItb4M4/sfQoUPVr18/NWvWTGFhYfr666+VmJioQYMGSbp+i+XEiROaPn26rFargoKCcmxfrVo1ubu73zQfAMzk5uymoWFDNaDJAP1n43/0xS9fKPpwtKIPR+vuSncrqnGU+gX3092V7jY7KlDq5HvMSGRkpD777DO9++67atKkiTZu3KgVK1aoVq1akqSkpKQ7vnMEAOxV5TKVNabDGP320m8a2GSgyruW1+ELhzVyw0jV/ry27p16ryb/OlnJV5LNjgqUGrwOHgBuIz0zXYt+W6Rpu6dpzeE1MnT9P5nuzu7q3qC7ooKj9NDdD8nJ6mRyUsD+8N00AFDIjqcc13d7vtO03dP029nfbPN9y/vqicZPqH9wfzWs1tDEhIB9oYwAQBExDEMxJ2M0bfc0zdo7S+cvn7cta+rbVP2D+6t3UG9VLVfVxJSA+SgjAFAMMrMytfzAck3bPU3LE5brWvY1SZKz1Vmd63XWW23fUvPqzU1OCZiDMgIAxezspbOaFTdL0/dMV8zJGNv8zvU665373lFT36YmpgOKH2UEAEy078w+fbztY03fPV3ZRrYkqVuDbhr5j5EK9gk2OR1QPIrki/IAAHnTsFpDTe06VfEvxuuJxk/IIosW/7ZYTSY2Uc95PbX3zF6zIwJ2gzICAEWoXpV6mtF9hva9sE+PBz0uiyyav3++Gk9orN4Leiv+z3izIwKmo4wAQDEIrBqoWY/O0p7n9+ixex6TIUOz985Ww/EN9cTCJ3Tg3AGzIwKmoYwAQDEKqhakeT3nKfa5WHVr0E2GDM2Mm6nALwM1YPEAHTp/yOyIQLGjjACACYJ9grUocpFinolR53qdlW1ka9ruaao/rr6eXvK0jl48anZEoNjwNA0A2IFfTvyiketHauXBlZKuv6eke4Pu6lKviyLqRsirrJfJCYH849FeACiBth3bphHrRyj6cLRtnkUWhfmHqXPdzupcr7OCqgXJYrGYmBLIG8oIAJRgMSdjtPi3xVp2YJl2n96dY1lNz5rqVLeTOtfrrPvvul9lXMqYlBK4PcoIAJQSx5KPaXnCci1PWK6fDv+kK9eu2JaVcS6jB+9+UJ3rdVanup1U3aO6iUmBnCgjAFAKXbp6SeuOrNOyA8u0LGGZjqccz7E8xCdEnetdv53T3K85t3NgKsoIAJRyhmFoz+k9Wp6wXMsOLNP249tl6P/+k16/Sn0NDBmoqOAo+ZT3MTEpHBVlBAAczJ/pf2rlwZVadmCZlics16WrlyRJThYndarXSU+FPKWIOhFycXIxOSkcBWUEABxYakaq5u6bq8m7Jmvb8W22+T7lfRTVOEoDQwaqvld9ExPCEVBGAACSpP1/7tfUXVM1fc90nUk/Y5vfxr+Nngp5Sj0b9lR51/ImJkRpRRkBAORwNeuqlics1+Rdk7UiYYWyjWxJUnnX8opsGKmnQp5SqxqtGPSKQkMZAQDk6mTqSU3fPV1Tdk1RwvkE2/xAr0ANDBmovo36yreCr4kJURpQRgAAd2QYhjYlbtKUXVM0b/8826BXiyxqVaOVujXopq71uzK+BAVCGQEA5EtKRorm7J2jKbFTtP349hzLGng1ULf63dStQTc1r95cVgvfs4o7o4wAAArsRMoJLfl9iX74/QetPbJWV7Ov2pb5lvfVI/UfUbcG3XT/XffLzdnNxKSwZ5QRAEChSL6SrJUHV2rxb4u1ImGFUjNTbcsquFZQRN0IdavfTR3rdpSnu6eJSWFvKCMAgEKXcS1D64+u1+LfFmvJgSU6mXrStszF6qL77rpP3Rp0U/cG3RkAC8oIAKBoZRvZtm8X/uH3H7T/z/22ZRZZ1K5WO/W8p6ceDXyUYuKgKCMAgGJ14NwB/fDbD1r428IcA2ApJo6LMgIAME1icqLm75+vefvnUUwcGGUEAGAXKCaOizICALA7FBPHQhkBANi13IqJ1WJV+4D26tuor3oE9pCHG//dL6koIwCAEuNGMZm7b65+PvGzbb67s7u61OuiPo36KKJOBC9YK2EoIwCAEunwhcP6Pu57zYybqd/O/mabX8m9kh675zH1bdRX7Wq145X0JQBlBABQohmGoV2ndun7uO81a++sHC9Y8/fwV++g3urTqI8aezeWxWIxMSlyQxkBAJQaWdlZ2vDHBs3cM1ML4hcoOSPZtqxh1Ybq26iv+jTqo1oVa5mYEn9FGQEAlEpXrl3RioQVmhk3U8sOLFNmVqZtWduabdW9QXd1rNtR9avU54qJyfL6+V2gG27jx49XQECA3N3dFRoaqk2bNuW67sKFC/XQQw+patWq8vDwUFhYmFavXl2Q3QIAIHdnd/UI7KEFvRbo9OunNfmRyWof0F4WWbQ5cbNe+/E1BX4ZqDpf1NHglYO16uAqXbl2xezYuI18XxmZM2eO+vXrp/Hjx6tNmzaaOHGivvnmG+3fv181a9a8af0hQ4bIz89P999/vypWrKipU6fq448/1s8//6yQkJA87ZMrIwCAOzmRckIL4hdoecJyrT+6PscVk7IuZfVAwAPqVLeTOtbtKH9PfxOTOo4iu03TsmVLNW3aVBMmTLDNCwwMVLdu3TRq1Kg8/Y6GDRsqMjJSw4cPz9P6lBEAQH6kZaZpzeE1Wp6wXCsSVuhE6okcyxtVa2QrJmH+YXK2OpuUtHTL6+d3vo5+Zmamdu7cqTfffDPH/PDwcG3dujVPvyM7O1upqamqXLlyrutkZGQoIyPD9nNKSkp+YgIAHFx51/Lq2qCrujboKsMwtOf0Hi1PWK7lCcu1/fh2xZ2JU9yZOH245UNVcq+kDnU6qGOdjoqoGyGvsl5mx3c4+SojZ8+eVVZWlry9vXPM9/b21qlTp/L0O8aMGaP09HT16tUr13VGjRqld955Jz/RAAC4JYvFomCfYAX7BOutdm/p3KVzWn1otZYnLNeqg6t0/vJ5zd47W7P3zpaz1VkRdSIUFRylzvU6y93Z3ez4DqFA16X+OjrZMIw8jVieNWuWRo4cqR9++EHVqlXLdb1hw4Zp6NChtp9TUlLk78/9PQDA31elbBX1adRHfRr1UVZ2ln4+8bOWH1iuZQnLtOf0Hi09sFRLDyxVRfeKimwYqX6N+6m1f2uezClC+SojXl5ecnJyuukqyJkzZ266WvJXc+bM0VNPPaV58+bpwQcfvO26bm5ucnPjlb8AgKLlZHVSa//Wau3fWv954D+K/zNeM/bM0Iw9M3Q85bgm7pyoiTsnqnal2urXuJ/6BffT3ZXuNjt2qZOvR3tdXV0VGhqq6OjoHPOjo6PVunXrXLebNWuWBgwYoO+//16dOnUqWFIAAIpYYNVAffDAB/pjyB9aE7VG/YP7q5xLOR26cEgjN4xU7c9rq93Udpq0c5IuXrlodtxSo8CP9n711VcKCwvT119/rUmTJmnfvn2qVauWhg0bphMnTmj69OmSrheRqKgojR07Vj169LD9njJlysjT0zNP++RpGgCAWdIz07Xot0Wavnu6fjr8kwxd/9h0c3LTI/UfUVRwlDrU7iAXJxeTk9qfIn0D6/jx4zV69GglJSUpKChIn376qe69915J0oABA3T06FGtX79eknTfffdpw4YNN/2O/v3769tvvy3UPwYAgKJ0IuWEvo/7XtN2T9O+P/fZ5lctW1V9GvVRVHCUQnxCGF/yX7wOHgCAImIYhmJPxWr67un6fu/3OpN+xrYsqFqQ+gf3V99GfeVbwdfElOajjAAAUAyuZl3Vj4d+1LTd07Tk9yXKyLr+niyrxaoOtTuof3B/dW3Q1SEfE6aMAABQzC5cvqA5++Zo+u7p2nZ8m22+p5unIhtGakCTAWpVo5XD3MahjAAAYKID5w5o+u7pmr57uo6lHLPNr1u5rvoH91e/4H6q6Xnzd7qVJpQRAADsQLaRrfVH12va7mmav3++Ll29JEmyyKL7A+5X/+D+6hHYQ+Vdy5uctPBRRgAAsDNpmWlasH+Bpu2epnVH19nml3Mpp+6B3dW5bmc9VPshVS6T+/e3lSSUEQAA7NgfF//QjD0zNG33NB08f9A232qxqlWNVnq49sOKqBuhpr5NZbXk6x2ldoMyAgBACWAYhrYd36ZF8Yu08uDKHO8vkaRq5aqpQ+0OiqgTofDa4apStopJSfOPMgIAQAmUmJyoVQdXaeXBlfrp8E9Ky0yzLbPIohbVWyiiToQi6kaomV8zu75qQhkBAKCEy8zK1NZjW7UyYaVWHlypuDNxOZZ7lfVSh9od9HCdh/VwnYflVdbLpKS3RhkBAKCUOZ5yPMdVk5SMFNsyq8WqsBph6lKvi7rU76JAr0DT32dCGQEAoBS7mnVV245v08qElVpxcIX2nN6TY/ndle6+XkzqddG9te415Yv8KCMAADiQxORELTuwTEsPLNXaI2uVmZVpW+bh5qGH6zysLvW6qGPdjsX26DBlBAAAB5WWmaboQ9FaemCplicsz/FFflaLVW3826hLvS56pP4jqu9Vv8hyUEYAAICyjWz9cuIXLf19qZYeWHrTINi6leuqc73OGtBkgBp7Ny7UfVNGAADATY5ePGq7nbP+6Hrb7ZzJj0zWwJCBhbovyggAALit1IxU/XjoRy09sFQfPfiRvMt7F+rvp4wAAABT5fXz235f2wYAABwCZQQAAJiKMgIAAExFGQEAAKaijAAAAFNRRgAAgKkoIwAAwFSUEQAAYCrKCAAAMBVlBAAAmIoyAgAATEUZAQAApqKMAAAAUzmbHSAvbnyxcEpKislJAABAXt343L7xOZ6bElFGUlNTJUn+/v4mJwEAAPmVmpoqT0/PXJdbjDvVFTuQnZ2tkydPqkKFCrJYLDmWpaSkyN/fX8eOHZOHh4dJCUsejlv+ccwKhuNWMBy3guG45V9RHjPDMJSamio/Pz9ZrbmPDCkRV0asVqtq1Khx23U8PDw48QqA45Z/HLOC4bgVDMetYDhu+VdUx+x2V0RuYAArAAAwFWUEAACYqsSXETc3N40YMUJubm5mRylROG75xzErGI5bwXDcCobjln/2cMxKxABWAABQepX4KyMAAKBko4wAAABTUUYAAICpKCMAAMBUJbqMjB8/XgEBAXJ3d1doaKg2bdpkdiS7NnLkSFkslhyTj4+P2bHszsaNG9WlSxf5+fnJYrFo8eLFOZYbhqGRI0fKz89PZcqU0X333ad9+/aZE9aO3Om4DRgw4Kbzr1WrVuaEtROjRo1S8+bNVaFCBVWrVk3dunXT77//nmMdzreb5eW4cb7lNGHCBDVu3Nj2YrOwsDCtXLnSttzs86zElpE5c+ZoyJAhevvtt7Vr1y61a9dOERERSkxMNDuaXWvYsKGSkpJsU1xcnNmR7E56erqCg4M1bty4Wy4fPXq0PvnkE40bN047duyQj4+PHnroIdt3KDmqOx03SXr44YdznH8rVqwoxoT2Z8OGDXrxxRe1fft2RUdH69q1awoPD1d6erptHc63m+XluEmcb/+rRo0a+vDDDxUTE6OYmBi1b99eXbt2tRUO088zo4Rq0aKFMWjQoBzzGjRoYLz55psmJbJ/I0aMMIKDg82OUaJIMhYtWmT7OTs72/Dx8TE+/PBD27wrV64Ynp6exldffWVCQvv01+NmGIbRv39/o2vXrqbkKSnOnDljSDI2bNhgGAbnW1799bgZBudbXlSqVMn45ptv7OI8K5FXRjIzM7Vz506Fh4fnmB8eHq6tW7ealKpkSEhIkJ+fnwICAvT444/r8OHDZkcqUY4cOaJTp07lOPfc3Nz0j3/8g3MvD9avX69q1aqpXr16euaZZ3TmzBmzI9mV5ORkSVLlypUlcb7l1V+P2w2cb7eWlZWl2bNnKz09XWFhYXZxnpXIMnL27FllZWXJ29s7x3xvb2+dOnXKpFT2r2XLlpo+fbpWr16tSZMm6dSpU2rdurXOnTtndrQS48b5xbmXfxEREZo5c6bWrl2rMWPGaMeOHWrfvr0yMjLMjmYXDMPQ0KFD1bZtWwUFBUnifMuLWx03ifPtVuLi4lS+fHm5ublp0KBBWrRoke655x67OM9KxLf25sZiseT42TCMm+bh/0RERNj+uVGjRgoLC1Pt2rU1bdo0DR061MRkJQ/nXv5FRkba/jkoKEjNmjVTrVq1tHz5cvXo0cPEZPbhpZde0p49e7R58+ablnG+5S6348b5drP69esrNjZWFy9e1IIFC9S/f39t2LDBttzM86xEXhnx8vKSk5PTTY3tzJkzNzU75K5cuXJq1KiREhISzI5SYtx4+ohz7+/z9fVVrVq1OP8kvfzyy1qyZInWrVunGjVq2OZzvt1ebsftVjjfJFdXV9WpU0fNmjXTqFGjFBwcrLFjx9rFeVYiy4irq6tCQ0MVHR2dY350dLRat25tUqqSJyMjQ/Hx8fL19TU7SokREBAgHx+fHOdeZmamNmzYwLmXT+fOndOxY8cc+vwzDEMvvfSSFi5cqLVr1yogICDHcs63W7vTcbsVzrebGYahjIwM+zjPimWYbBGYPXu24eLiYkyePNnYv3+/MWTIEKNcuXLG0aNHzY5mt1577TVj/fr1xuHDh43t27cbnTt3NipUqMAx+4vU1FRj165dxq5duwxJxieffGLs2rXL+OOPPwzDMIwPP/zQ8PT0NBYuXGjExcUZvXv3Nnx9fY2UlBSTk5vrdsctNTXVeO2114ytW7caR44cMdatW2eEhYUZ1atXd+jj9vzzzxuenp7G+vXrjaSkJNt06dIl2zqcbze703HjfLvZsGHDjI0bNxpHjhwx9uzZY7z11luG1Wo1fvzxR8MwzD/PSmwZMQzD+PLLL41atWoZrq6uRtOmTXM81oWbRUZGGr6+voaLi4vh5+dn9OjRw9i3b5/ZsezOunXrDEk3Tf379zcM4/rjliNGjDB8fHwMNzc349577zXi4uLMDW0HbnfcLl26ZISHhxtVq1Y1XFxcjJo1axr9+/c3EhMTzY5tqlsdL0nG1KlTbetwvt3sTseN8+1mAwcOtH1eVq1a1XjggQdsRcQwzD/PLIZhGMVzDQYAAOBmJXLMCAAAKD0oIwAAwFSUEQAAYCrKCAAAMBVlBAAAmIoyAgAATEUZAQAApqKMAAAAU1FGAACAqSgjAADAVJQRAABgKsoIAAAw1f8Hf43LZldKypcAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "c:\\Users\\legoe\\miniconda3\\envs\\RL-DT-ENV\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inertia: 0.9540020463324588\n",
            "These should match if our classification is correct:\n",
            "Romeo and Juliet: [4]\n",
            "Othello: [4]\n",
            "And they do!\n",
            "These should not match:\n",
            "Coriolanus: [0]\n",
            "Twelfth Night [4]\n",
            "However, Twelfth night is a comedy that doesn't matches the tragedies. Clearly the model isn't perfect.\n",
            "Reducing the components reduces the overall error substantially:\n",
            "Inertia: 0.8104784897221186\n",
            "These should match:\n",
            "Romeo and Juliet: [4]\n",
            "Othello: [4]\n",
            "Is Twelfth Night (Comedy) distinct from Romeo and Juliet and Othello?\n",
            "Twelfth Night: [4]\n",
            "No, PCA was not able to fix this discrepancy.\n"
          ]
        }
      ],
      "source": [
        "# Extra Imports\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Firstly, lets clean the data because I notice a row that is not supposed to be there, the .ipynb_checkpoints\n",
        "df = shakespeare.drop(\".ipynb_checkpoints\")\n",
        "\n",
        "# Creating the X \n",
        "X = np.array(df, dtype=np.float64)\n",
        "\n",
        "# Making the model, standard parameters for now\n",
        "# Using n_clusters=5 because of the 5 types of plays: Comedy, History, Tragedy, Romance, and Tragicomedy\n",
        "model = KMeans(n_clusters=5, random_state=42)\n",
        "\n",
        "# Training the model\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Line to fix memory leak warning\n",
        "model.fit(X)\n",
        "\n",
        "inertia = np.array([])\n",
        "n_clusters = np.arange(1, 31)\n",
        "\n",
        "for i in n_clusters:\n",
        "    model = KMeans(n_clusters=i, random_state=42)\n",
        "    model.fit(X)\n",
        "    inertia = np.append(inertia, model.inertia_)\n",
        "\n",
        "# Plotting inertia vs n_clusters\n",
        "plt.plot(n_clusters, inertia, color=\"green\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Both Romeo and Juliet and Othello are classfied as tragedies according to Folgers Shakespear\n",
        "Thus, they should be in the same genre cluster\n",
        "\"\"\"\n",
        "\n",
        "model = KMeans(n_clusters=5, random_state=42)\n",
        "model.fit(X)\n",
        "print(\"Inertia:\", model.inertia_)\n",
        "print(\"These should match if our classification is correct:\")\n",
        "print(\"Romeo and Juliet:\", model.predict(np.array([df.loc[\"romeo-and-juliet\"]])))\n",
        "print(\"Othello:\", model.predict(np.array([df.loc[\"othello\"]])))\n",
        "print(\"And they do!\")\n",
        "\n",
        "print(\"These should not match:\")\n",
        "print(\"Coriolanus:\", model.predict(np.array([df.loc[\"coriolanus\"]])))\n",
        "print(\"Twelfth Night\", model.predict(np.array([df.loc[\"twelfth-night\"]])))\n",
        "print(\"However, Twelfth night is a comedy that doesn't matches the tragedies. Clearly the model isn't perfect.\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Implementing PCA to reduce the dimensionality of the words\n",
        "\"\"\"\n",
        "pca = PCA(n_components=15)\n",
        "X = pca.fit_transform(X)\n",
        "model = KMeans(n_clusters=5, random_state=42)\n",
        "model.fit(X)\n",
        "\n",
        "print(\"Reducing the components reduces the overall error substantially:\")\n",
        "print(\"Inertia:\", model.inertia_)\n",
        "\n",
        "# These are the necessary indices, we need to use the transformed data to predict\n",
        "print(\"These should match:\")\n",
        "print(\"Romeo and Juliet:\", model.predict([X[28]]))\n",
        "print(\"Othello:\", model.predict([X[27]]))\n",
        "\n",
        "print(\"Is Twelfth Night (Comedy) distinct from Romeo and Juliet and Othello?\")\n",
        "print(\"Twelfth Night:\", model.predict([X[33]]))\n",
        "print(\"No, PCA was not able to fix this discrepancy.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5vAkq0VH2cX"
      },
      "source": [
        "# Questions\n",
        "\n",
        "In addition to the questions, above, list your answer to this question:  \n",
        "\n",
        "1. If the common words list does not seem to tell us much about genre, what methods can you find online for extracting unimportant words from the text files. What is that method?\n",
        "\n",
        "2. Bonus: Implement one method you found above in the block below. Did it improve your results?\n",
        "\n",
        "3. How might K-means clustering help you to solve a timeless or new problem in your field, or a field besides literary studies?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. If we wanted to eliminate unimportant information from the words to reduce the amount of noise or limit the training time, we could use a technique like\n",
        "Principle Component Analysis to extract the most important words from the dataset and throw away words that are more uniform across plays.\n",
        "2. I implemented the PCA method in the above cell before training the model. I decreased from 100 parameters to 15 and found that it decreased the inertia \n",
        "but had a limited effect on the accuracy of the classifications. It is possible that if I tested more plays, I would find that it improved the results, but it\n",
        "yielded similar results for the few plays I considered.\n",
        "3. I am particularly interested in automation, and automated quality control for manufactured products is a huge part of building an automated assembly line. \n",
        "K-Means and PCA could be used to cluster images of parts, adjust for rotation, and find parts that were outliers in a product line. While it is unlikely that \n",
        "K-Means would be the core machine learning algorithm used in such a system, it could be an important of the image preprocessing and encoding stage, for instance\n",
        "to align part images to a central rotation for another neural network to detect failures or defects.\n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rubric\n",
        "\n",
        "| Criteria | Failing                                    | Passing                                                                                                | Reaching Full Credit                                               |\n",
        "|----------|--------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------|\n",
        "|     Completing Notebook     | No attempt at notebook or creating a model | Attempt at notebook and creating a model and all non-bonus questions answered | Meet Passing Criteria with all bonus questions and tasks answered. |\n",
        "\n",
        "If any criteria are “Failing” |  50% Maximum\n",
        "All criteria “Passing” | 70%\n",
        "For each “Reaching Full Credit” criteria met | + 30%\n",
        "\n",
        "Maximum Score: 100%\n",
        "\n",
        "Note that a lack of quality of questions answered may result in a reduced score.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RL-DT-ENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
